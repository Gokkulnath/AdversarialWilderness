{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Post 0: Adversarial Machine Learning Paper Reading Challenge\"\n",
    "> \"A Post with list of paper for Paper Reading Challenge\"\n",
    "- toc: true\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [\"Adversarial Machine Learning\"]\n",
    "- image: images/notebook/AMLPaperReadingChallenge.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summary Series Road-map :\n",
    "Top 30 Key Research papers to get the necessary fundamental papers that anyone who wants to perform neural network evaluations should read. The papers are split by topic and indicated which topics should be read before others. (List shared by Nicholas Carlini )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Papers\n",
    "- [ ] Evasion Attacks against Machine Learning at Test Time\n",
    "- [ ] Intriguing properties of neural networks\n",
    "- [ ] Explaining and Harnessing Adversarial Examples\n",
    "\n",
    "### Attacks [requires Preliminary Papers]\n",
    "- [ ] The Limitations of Deep Learning in Adversarial Settings\n",
    "- [ ] DeepFool: a simple and accurate method to fool deep neural networks\n",
    "- [ ] Towards Evaluating the Robustness of Neural Networks\n",
    "\n",
    "### Transferability [requires Preliminary Papers]\n",
    "- [ ] Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples\n",
    "- [ ] Delving into Transferable Adversarial Examples and Black-box Attacks\n",
    "- [ ] Universal adversarial perturbations\n",
    "\n",
    "### Detecting Adversarial Examples [requires Attacks, Transferability]\n",
    "\n",
    "- [ ] On Detecting Adversarial Perturbations\n",
    "- [ ] Detecting Adversarial Samples from Artifacts\n",
    "- [ ] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods\n",
    "\n",
    "### Restricted Threat Model Attacks [requires Attacks]\n",
    "- [ ] ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models\n",
    "- [ ] Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\n",
    "- [ ] Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors\n",
    "\n",
    "### Physical-World Attacks [reqires Attacks, Transferability]\n",
    "- [ ] Adversarial examples in the physical world\n",
    "- [ ] Synthesizing Robust Adversarial Examples\n",
    "- [ ] Robust Physical-World Attacks on Deep Learning Models\n",
    "\n",
    "### Verification [requires Introduction]\n",
    "\n",
    "- [ ] Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks\n",
    "- [ ] On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models\n",
    "\n",
    "### Defenses (2) [requires Detecting]\n",
    "- [ ] Towards Deep Learning Models Resistant to Adversarial Attacks\n",
    "- [ ] Certified Robustness to Adversarial Examples with Differential Privacy\n",
    "\n",
    "### Attacks (2) [requires Defenses (2)]\n",
    "- [ ] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\n",
    "- [ ] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks\n",
    "\n",
    "### Defenses (3) [requires Attacks (2)]\n",
    "- [ ] Towards the first adversarially robust neural network model on MNIST\n",
    "- [ ] On Evaluating Adversarial Robustness\n",
    "\n",
    "### Other Domains [requires Attacks]\n",
    "- [ ] Adversarial Attacks on Neural Network Policies\n",
    "- [ ] Audio Adversarial Examples: Targeted Attacks on Speech-to-Text\n",
    "- [ ] Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples\n",
    "- [ ] Adversarial examples for generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visualized using Adversarial Machine Learning Reading List by Nicholas Carlini](resources/AMLPaperReading.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualized using Adversarial Machine Learning Reading List by Nicholas Carlini — [Link](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Researchers  their affiliations:\n",
    "\n",
    "- Nicholas Carlini (Google Brain)\n",
    "- Anish Athalye (MIT)\n",
    "- Nicolas Papernot (Google Brain)\n",
    "- Wieland Brendel (University of Tubingen)\n",
    "- Jonas Rauber (University of Tubingen)\n",
    "- Dimitris Tsipras (MIT)\n",
    "- Ian Goodfellow (Google Brain)\n",
    "- Aleksander Madry (MIT)\n",
    "- Alexey Kurakin (Google Brain)\n",
    "\n",
    "### Research Labs :\n",
    "\n",
    "- Bethge Lab : http://bethgelab.org/\n",
    "\n",
    "### Frameworks/Libraries:\n",
    "\n",
    "- cleverhans- Tensorflow\n",
    "- foolbox — Keras/Tensorflow/Pytorch\n",
    "- advertorch — Pytorch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
