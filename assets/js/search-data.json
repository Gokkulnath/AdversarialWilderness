{
  
    
        "post0": {
            "title": "_2020 01 01 Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/2020/05/22/_2020-01-01-Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/05/22/_2020-01-01-Microsoft-Word-Example-Post.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A Tour On Information Exposure Modern World",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/2020/05/19/A-Tour-on-Information-Exposure-Modern-World.html",
            "relUrl": "/2020/05/19/A-Tour-on-Information-Exposure-Modern-World.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Network Adversary Generator",
            "content": "Introduction . Paper Abstract: . Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations making it very hard to defend. . Motivation . Current Approaches for crafting adversaries for a given classifier generate only one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. In order to build robust models, it is essential to explore diverse manifold of adversarial perturbations. This work can be of very useful, when we are using adversarial trainning, where the cost of generation of adversaries is high(Depends on the attack). With this approach, we will be able to generate adversarial noises from the learned distribution of adversarial perturbations. . Key Results: . The author&#39;s demonstrate that perturbations crafted by this model . achieve state-of-the-art fooling rates | exhibit wide variety | deliver excellent cross model generalizability. | Aproach . The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. The trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. . . Core idea is to model the distribution of universal adversarial perturbations for a given classifier. | The image shows a batch of B random vectors {z}B transforming into perturbations {delta}B by G which get added to the batch of data samples {x}B. | The top portion shows adversarial batch (XA), bottom portion shows shuffled adversarial batch (XS) and middle portion shows the benign batch (XB). The Fooling objective Lf and Diversity objective Ld constitute the loss. | Note: The target CNN (f) is a trained classifier and its parameters are not updated during the proposed training. On the other hand, the parameters of generator (G) are randomly initialized and learned through backpropagating the loss. (Best viewed in color). | . Code: . Data Preparation . Download of Dataset P.S: Randomly Sampled 10 instances from each target class as described in the paper. | Option 1: Download from Archive.org Archive Link | train.zip | valid.zip | . | Option 2 : Mega Download Link for Train abd Validation data of Imagenet 2012 (Obtained from Kaggle) Validation Data: Mega Link | Trainning Data: Mega Link | . | Setting up of Folder Structure For Easier handling and reproducibility of results download from mega link | . Code Below Assumes Dataset is downlaoded and setup . Verify Dataset . #collapse-hide from glob import glob train_ok = True val_ok = True print(&quot;Training Data Verification&quot;) cls_count = len(glob(&quot;ILSVRC/train/*&quot;)) print(&quot;Total Number of Classes: {} in train directory&quot;.format(cls_count)) count = 0 for cls_ in glob(&quot;ILSVRC/train/*&quot;): imgs = glob(cls_ + &quot;/*&quot;) img_count = len(imgs) count += img_count if img_count != 10: print(cls_.split(&quot;/&quot;)[-1], img_count) train_ok=False print(&quot;Total {} number of files in {} classes. i.e 10 Images/Class&quot;.format(count, cls_count)) print(&quot;Validation Data Verification&quot;) val_files = glob(&quot;ILSVRC/valid/*&quot;) val_count = len(val_files) if val_count == 50000: print(&quot;Validation Data has correct number of files i.e {}&quot;.format(val_count)) else: print(&quot;Validation Data has some issue. Has following number of file : {}. Kindly Check!!&quot;.format(val_count)) val_ok=False if train_ok and val_ok: print(&quot;Dataset is Setup Correctly&quot;) . . Training Data Verification Total Number of Classes: 1000 in train directory Total 10000 number of files in 1000 classes. i.e 10 Images/Class Validation Data Verification Validation Data has correct number of files i.e 50000 Dataset is Setup Correctly . Imports . #collapse-hide import torch import torch.nn as nn from torch import optim import torch.nn.functional as F from torch.utils.data import DataLoader,Dataset import torchvision import torchvision.models as tvm from torchvision import transforms from torchvision.datasets.folder import DatasetFolder,ImageFolder import numpy as np from glob import glob from PIL import Image import pandas as pd import os,time,gc from pathlib import Path from tqdm import tqdm_notebook as tqdm import datetime,random,string . . ngpu=torch.cuda.device_count() device = torch.device(&quot;cuda&quot; if (torch.cuda.is_available() and ngpu &gt; 0) else &quot;cpu&quot;) print(&quot;Using Pytorch Version : {} and Torchvision Version : {}. Using Device {}&quot;.format(torch.__version__,torchvision.__version__,device)) . Using Pytorch Version : 1.4.0 and Torchvision Version : 0.5.0. Using Device cuda . Dataset and Dataloaders Setup . dataset_path=r&#39;ILSVRC/&#39; train_dataset_path=dataset_path+&#39;train&#39; test_dataset_path=dataset_path+&#39;valid&#39; print(&quot;Dataset root Folder:{}. Train Data Path: {}. Validation Data Path {}&quot;.format(dataset_path,train_dataset_path,test_dataset_path)) . Dataset root Folder:ILSVRC/. Train Data Path: ILSVRC/train. Validation Data Path ILSVRC/valid . # Preparation of Labels label_dict={} label_idx={} with open(&#39;ILSVRC/LOC_synset_mapping.txt&#39;) as file: lines=file.readlines() for idx,line in enumerate(lines): label,actual =line.strip(&#39; n&#39;).split(&#39; &#39;,maxsplit=1) label_dict[label]=actual label_idx[label]=idx . Transforms . # transforms size=224 # Imagenet Stats vgg_mean = [103.939, 116.779, 123.68] preprocess=transforms.Compose([transforms.Resize((size,size)), transforms.ToTensor(), transforms.Normalize(vgg_mean,(0.5, 0.5, 0.5))]) . Dataset and Dataloaders . class CustomDataset(Dataset): def __init__(self, subset, root_dir, transform=None): self.root_dir=root_dir self.transform=transform self.subset=subset if self.subset==&#39;train&#39;: data_dir=os.path.join(self.root_dir,self.subset) self.images_fn=glob(f&#39;{data_dir}/*/*&#39;) self.labels=[Path(fn).parent.name for fn in self.images_fn] elif subset ==&#39;valid&#39;: df=pd.read_csv(&#39;ILSVRC/LOC_val_solution.csv&#39;) df[&#39;label&#39;]=df[&#39;PredictionString&#39;].str.split(&#39; &#39;,n=1,expand=True)[0] df=df.drop(columns=[&#39;PredictionString&#39;]) self.images_fn=&#39;ILSVRC/valid/&#39;+df[&#39;ImageId&#39;].values+&#39;.JPEG&#39; self.labels=df[&#39;label&#39;] else: raise ValueError print(f&quot; Number of instances in {self.subset} subset of Dataset: {len(self.images_fn)}&quot;) def __getitem__(self,idx): fn=self.images_fn[idx] label=self.labels[idx] image=Image.open(fn) if image.getbands()[0] == &#39;L&#39;: image = image.convert(&#39;RGB&#39;) if self.transform: image = self.transform(image) return image,label_idx[label] def __len__(self): return len(self.images_fn) data_train=ImageFolder(root=&#39;ILSVRC/train&#39;,transform=preprocess) class2idx=data_train.class_to_idx data_valid=CustomDataset(subset=&#39;valid&#39;,root_dir=dataset_path,transform=preprocess) train_num = len(data_train) val_num = len(data_valid) . Number of instances in valid subset of Dataset: 50000 . Loss Functions/Objectives . def fooling_objective(qc_): &#39;&#39;&#39;Helper function to computer compute -log(1-qc&#39;), where qc&#39; is the adversarial probability of the class having maximum probability in the corresponding clean probability qc&#39; &gt; qc_ Parameters: prob_vec : Probability vector for the clean batch adv_prob_vec : Probability vecotr of the adversarial batch Returns: -log(1-qc&#39;) , qc&#39; &#39;&#39;&#39; # Get the largest probablities from predictions : Shape (bs,1) qc_=qc_.mean() return -1*torch.log(1-qc_) , qc_ def diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled): &#39;&#39;&#39;Helper function to calculate the cosine distance between two probability vectors Parameters: prob_vec : Probability vector for the clean batch adv_prob_vec : Probability vector for the adversarial batch Returns : Cosine distance between the corresponding clean and adversarial batches &#39;&#39;&#39; return torch.cosine_similarity(prob_vec_no_shuffle,prob_vec_shuffled).mean() ## TODO : Not Required. As we always take the last layer. def intermediate_activation_objective(layer_name=None): &#39;&#39;&#39; Extract the activations of any intermediate layer for: 1. batch of images (of batch size=32) corrupted by the perturbations (of batch size=32) 2. same batch of images corrupted by same batch of perturbations but in different (random) order (in this case the intermdeiate layer is set to &#39;res4f&#39; of ResNet 50 architecture) &#39;&#39;&#39; if arch ==&#39;resnet50&#39;: layer_name=&#39;res4f&#39; pass . Self Note: . Effect of ConvTranspose2d : It is a combination of upsampling and convolution layers used to increase the spatial resolution of the tensor | . Generator . Architecture of the generator (G) unchanged for different target CNN architectures | . . from torch import nn ngf=128 nz= latent_dim=10 e_lim = 10 nc=3 # Number of Channels # Fixed Architecture: Weights will be updated by Backprop. class AdveraryGenerator(nn.Module): def __init__(self,e_lim): super(AdveraryGenerator, self).__init__() self.e_lim = e_lim self.main = nn.Sequential( nn.ConvTranspose2d( in_channels=nz,out_channels= 1024, kernel_size=4, stride=1, padding=0, bias=False), nn.BatchNorm2d(1024), nn.ReLU(True), # state size. (ngf*8) x 4 x 4 nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True), # state size. (ngf*4) x 8 x 8 nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True), # state size. (ngf*2) x 16 x 16 nn.ConvTranspose2d(256, 128, 4, 2, 2, bias=False), nn.BatchNorm2d(128), nn.ReLU(True), # state size. (ngf) x 32 x 32 nn.ConvTranspose2d( 128, 64, 4, 2, 2, bias=False), nn.BatchNorm2d(64), nn.ReLU(True), # state size. (nc) x 64 x 64 nn.ConvTranspose2d( 64, 3, 4, 4,4, bias=False), nn.BatchNorm2d(3), nn.ReLU(True), nn.Tanh() ) def forward(self, x): return self.e_lim * self.main(x) # Scaling of ε # move Generator to GPU if available adversarygen=AdveraryGenerator(e_lim).to(device) . Debugging . #collapse-hide if debug: try: from torchsummary import summary summary(adversarygen,(nz,1,1)) except: raise(&#39;Check torchsummary is installed. If not install using the command pip install torchsummary&#39;) . . Setting up Discriminator : Model : Architecture . from torchvision.models import googlenet, vgg16 , vgg19, resnet152, resnet50 model_dict ={ &#39;googlenet&#39;: googlenet, &#39;vgg16&#39;: vgg16 , &#39;vgg19&#39;:vgg19, &#39;resnet152&#39;:resnet152, # TODO Generate Perturbations &#39;resnet50&#39;:resnet50 # TODO Generate Perturbations } . Run only once : . #collapse-hide # Get all Pretrained Weights: for arch in model_dict.keys(): if arch !=&#39;vgg-f&#39;: model=model_dict[arch](pretrained=True) . . Choice of Hyperparameters . The architecture of the generator consists of 5 deconv layers. The final deconv layer is followed by a tanh non-linearity and scaling by epsillon (10) | . # epsillon=10 # batch_size=32 # latent_dim = 10 img_h,img_w,img_c=(224,224,3) latent_dim=10 arch=&#39;resnet50&#39; archs=model_dict.keys() # [&#39;vgg-f&#39;,&#39;vgg16&#39;,&#39;vgg19&#39;,&#39;googlenet&#39;,&#39;resnet50&#39;,&#39;resnet152&#39;] def get_bs(arch): if torch.cuda.is_available(): # GPU_BENCHMARK= 8192.0 # GPU_MAX_MEM = torch.cuda.get_device_properties(device).total_memory / (1024*1024) # BS_DIV= GPU_BENCHMARK/GPU_MAX_MEM # print(f&quot;Current GPU MAX Size : {GPU_MAX_MEM}. {BS_DIV}&quot;) if arch not in [&#39;resnet50&#39;,&#39;resnet152&#39;]:# [&#39;vgg16&#39;,&#39;vgg19&#39;,&#39;vgg-f&#39;,&#39;googlenet&#39;]: bs=int(64) elif arch in [&#39;resnet50&#39;,&#39;resnet152&#39;]: bs=int(32) else: raise ValueError(f&#39;Architecture type not supported. Please choose one from the following {archs}&#39;) else: bs=8 # OOM Error return bs get_bs(arch) . 32 . model=model_dict[arch](pretrained=True) model . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=2048, out_features=1000, bias=True) ) . Other Utils . #collapse-hide def save_checkpoint(model, to_save, filename=&#39;checkpoint.pth&#39;): &quot;&quot;&quot;Save checkpoint if a new best is achieved&quot;&quot;&quot; if to_save: print (&quot;=&gt; Saving a new best&quot;) torch.save(model.state_dict(), filename) # save checkpoint else: print (&quot;=&gt; Validation Accuracy did not improve&quot;) def save_perturbations(noise,arch,epoch,wabdb_flag=False): rand_str= &#39;&#39;.join( random.choice(string.ascii_letters) for i in range(6)) os.makedirs(f&quot;{arch}-{rand_str}&quot;,exist_ok=True) perturbations=noise.permute(0,2,3,1).cpu().detach().numpy()*255 np.save(f&#39;{arch}-{rand_str}/Perturbations_{arch}_{epoch}.npy&#39;, perturbations) for perturb_idx,perturbation in enumerate(perturbations[:,]): im = Image.fromarray(perturbation.astype(np.uint8)) if wabdb_flag: wandb.log({&quot;noise&quot;: [wandb.Image(im, caption=f&quot;Noise_{arch}_{epoch}_{perturb_idx}&quot;)]}) im.save(f&#39;{arch}-{rand_str}/Perturbations_{arch}_{epoch}_{perturb_idx}.png&#39;) # TODO def visualize_perturbations(): # MAtplotlib Subplot ? # Subplots(4*4) or (3*3) # From Memory or Disk - Epoch number ? pass def get_preds(predictions,return_idx=False, k=1): idxs= torch.argsort(predictions,descending=True)[:,:k] if return_idx: return predictions[:,idxs], idxs return predictions[:,idxs] . . Validating Model Utils . #collapse-hide # val_iterations = val_num/bs def compute_fooling_rate(prob_adv,prob_real): &#39;&#39;&#39;Helper function to calculate mismatches in the top index vector for clean and adversarial batch Parameters: prob_adv : Index vector for adversarial batch prob_real : Index vector for clean batch Returns: Number of mismatch and its percentage &#39;&#39;&#39; nfool=0 size = prob_real.shape[0] for i in range(size): if prob_real[i]!=prob_adv[i]: nfool = nfool+1 return nfool, 100*float(nfool)/size def validate_generator_old(noise,val_dl,val_iterations=10): total_fool=0 print(&quot;############### VALIDATION PHASE STARTED ################&quot;) train_log.writelines(&quot;############### VALIDATION PHASE STARTED ################&quot;) for val_idx in range(val_iterations): for batch_idx, data in enumerate(val_dl): images = data[0].to(device) # labels = data[1].to(device) prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q prob_vec_no_shuffle = D_model(images + noise) nfool, _ = compute_fooling_rate(prob_vec_no_shuffle,prob_vec_clean) total_fool += nfool fool_rate = 100*float(total_fool)/(val_iterations*batch_size) print(f&quot;Fooling rate: {foolr}. Total Items Fooled :{total_fool}&quot;) train_log.writelines(f&quot;Fooling rate: {foolr}. Total Items Fooled :{total_fool}&quot;) def validate_generator(noise,D_model,val_dl): total_fool=0 for batch_idx, data in tqdm(enumerate(val_dl),total = val_num//val_dl.batch_size): val_images = data[0].to(device) val_labels = data[1].to(device) prob_vec_clean,clean_idx = get_preds(F.softmax(D_model(val_images),dim=0),return_idx=True) # Variable q prob_vec_no_shuffle,adv_idx = get_preds(F.softmax(D_model(val_images + noise),dim=0),return_idx=True) nfool, _ = compute_fooling_rate(adv_idx,clean_idx) total_fool += nfool fool_rate = 100*float(total_fool)/(val_num) return fool_rate,total_fool . . Setup Wandb . Fit and Train the Generator . def fit(nb_epochs,D_model,dls,optimizer,adversarygen=adversarygen): # Set the Discriminator in Eval mode; Weights are fixed. train_dl,val_dl = dls D_model=D_model.to(device) D_model.eval() timestamp=datetime.datetime.now().strftime(&quot;%d%b%Y_%H_%M&quot;) train_log = open(f&#39;train_log_{arch}_{timestamp}.txt&#39;,&#39;w&#39;) for epoch in tqdm(range(nb_epochs),total=nb_epochs): running_loss=0 rand_str= &#39;&#39;.join( random.choice(string.ascii_letters) for i in range(6)) train_log.writelines(f&quot;############### TRAIN PHASE STARTED : {epoch}################&quot;) for batch_idx, data in tqdm(enumerate(train_dl),total = train_num//train_dl.batch_size): # Move Data and Labels to device(GPU) images = data[0].to(device) labels = data[1].to(device) # Generate the Adversarial Noise from Uniform Distribution U[-1,1] latent_seed = 2 * torch.rand(bs, nz, 1, 1, device=device,requires_grad=True) -1 # (r1 - r2) * torch.rand(a, b) + r2 noise = adversarygen(latent_seed) optimizer.zero_grad() # XB = images #preds_XB = f(images) prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q clean_preds ,clean_idx = get_preds(prob_vec_clean,return_idx=True,k=1) #XA = images+noise #preds_XA = f(images + noise) prob_vec_no_shuffle = D_model(images + noise) qc_ = F.softmax(prob_vec_no_shuffle,dim=0).gather(1,clean_idx) # Variable q&#39;c # 1. fooling_objective: encourages G to generate perturbations that decrease confidence of benign predictions fool_obj, mean_qc_ = fooling_objective(qc_) # Perturbations are shuffled across the batch dimesion to improve diversity #XS = images+ noise[torch.randperm(bs)] prob_vec_shuffled = D_model(images + noise[torch.randperm(bs)]) # 2. encourages Generator to explore the space of perturbations and generate a diverse set of perturbations divesity_obj=diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled) # Compute Total Loss total_loss = divesity_obj + fool_obj # Lets perform Backpropagation to compute Gradients and update the weights total_loss.backward() optimizer.step() # wandb Logging : Expensive : Logs Perturbation Images each iteration # perturbations=noise.permute(0,2,3,1).cpu().detach().numpy()*255 # for perturb_idx,perturbation in enumerate(perturbations[:,]): # im = Image.fromarray(perturbation.astype(np.uint8)) # wandb.log({&quot;noise&quot;: [wandb.Image(im, caption=f&quot;Noise_{arch}_{epoch}_{perturb_idx}&quot;)]}) wandb.log({&quot;fool_obj&quot;: fool_obj.item(), &quot;divesity_obj&quot;: divesity_obj.item(), &quot;total_loss&quot;:total_loss.item(), }) running_loss += total_loss.item() if batch_idx!=0 and batch_idx % 100 ==0 : train_log.writelines(f&quot;############### VALIDATION PHASE STARTED : {epoch}, Step : {int(batch_idx / 100)} ################&quot;) fool_rate,total_fool= validate_generator(noise,D_model,val_dl) print(f&quot;Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}&quot;) train_log.writelines(f&quot;Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}&quot;) print(f&quot;Diversity Loss :{divesity_obj.item()} n Fooling Loss: {fool_obj.item()} n&quot;) print(f&quot;Total Loss after Epoch No: {epoch +1} - {running_loss/(train_num//train_dl.batch_size)}&quot;) train_log.writelines(f&quot;Loss after Epoch No: {epoch +1} is {running_loss/(train_num//train_dl.batch_size)}&quot;) # to_save can be any expression/condition that returns a bool save_checkpoint(adversarygen, to_save= True, filename=f&#39;GeneratorW_{arch}_{epoch}_{rand_str}.pth&#39;) if epoch % 1 == 0: # save_perturbations(noise,arch,epoch) save_perturbations(noise,arch,epoch,wabdb_flag=True) train_log.close() . Start Actual Trainning . total_epochs = 20 lr = 1e-3 # Setting up Dataloaders import time,gc arch=&#39;resnet50&#39; start= time.time() print(f&quot;Training Generator for Arch {arch}&quot;) model= model_dict[arch](pretrained=True) bs = get_bs(arch) print(bs) train_dl=DataLoader(data_train,batch_size=bs,shuffle=True,num_workers=4,pin_memory=True,drop_last=True) val_dl=DataLoader(data_valid,batch_size=bs,shuffle=True,num_workers=4,pin_memory=True,drop_last=True) dls = [train_dl,val_dl] optimizer = optim.Adam(adversarygen.parameters(), lr=lr) print(f&quot;Elsasped Time {time.time()-start} Seconds&quot;) . Training Generator for Arch resnet50 32 Elsasped Time 0.6291134357452393 Seconds . fit(nb_epochs=total_epochs,D_model=model,dls=dls,optimizer=optimizer) . /home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` /home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` del sys.path[0] /home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . Fooling rate: 99.126. Total Items Fooled :49563 Fooling rate: 99.506. Total Items Fooled :49753 Fooling rate: 99.364. Total Items Fooled :49682 Diversity Loss :0.9992280602455139 Fooling Loss: 0.03950555995106697 Total Loss after Epoch No: 1 - 1.0430664758269603 =&gt; Saving a new best Fooling rate: 99.332. Total Items Fooled :49666 Fooling rate: 99.486. Total Items Fooled :49743 Fooling rate: 98.67. Total Items Fooled :49335 Diversity Loss :0.9981168508529663 Fooling Loss: 0.02838512323796749 Total Loss after Epoch No: 2 - 1.039582596757473 =&gt; Saving a new best Fooling rate: 99.446. Total Items Fooled :49723 Fooling rate: 99.52. Total Items Fooled :49760 Fooling rate: 99.38. Total Items Fooled :49690 Diversity Loss :0.9990019798278809 Fooling Loss: 0.004721864592283964 Total Loss after Epoch No: 3 - 1.039885509854708 =&gt; Saving a new best Fooling rate: 99.356. Total Items Fooled :49678 Fooling rate: 99.658. Total Items Fooled :49829 Fooling rate: 99.662. Total Items Fooled :49831 Diversity Loss :0.999189555644989 Fooling Loss: 0.003563906066119671 Total Loss after Epoch No: 4 - 1.0383393155076566 =&gt; Saving a new best Fooling rate: 99.62. Total Items Fooled :49810 Fooling rate: 99.486. Total Items Fooled :49743 Fooling rate: 99.686. Total Items Fooled :49843 Diversity Loss :0.9990421533584595 Fooling Loss: 0.008808250539004803 Total Loss after Epoch No: 5 - 1.0367735035908527 =&gt; Saving a new best Fooling rate: 99.744. Total Items Fooled :49872 Fooling rate: 99.338. Total Items Fooled :49669 Fooling rate: 99.692. Total Items Fooled :49846 Diversity Loss :0.9993197917938232 Fooling Loss: 0.005549242720007896 Total Loss after Epoch No: 6 - 1.037304274737835 =&gt; Saving a new best Fooling rate: 99.494. Total Items Fooled :49747 Fooling rate: 99.422. Total Items Fooled :49711 Fooling rate: 99.454. Total Items Fooled :49727 Diversity Loss :0.9981052875518799 Fooling Loss: 0.04303847253322601 Total Loss after Epoch No: 7 - 1.038387955763401 =&gt; Saving a new best Fooling rate: 99.29. Total Items Fooled :49645 Fooling rate: 99.416. Total Items Fooled :49708 Fooling rate: 99.558. Total Items Fooled :49779 Diversity Loss :0.998153567314148 Fooling Loss: 6.139297056506621e-06 Total Loss after Epoch No: 8 - 1.0364860896116648 =&gt; Saving a new best Fooling rate: 99.59. Total Items Fooled :49795 Fooling rate: 99.78. Total Items Fooled :49890 Fooling rate: 99.444. Total Items Fooled :49722 Diversity Loss :0.998258113861084 Fooling Loss: 0.002013623248785734 Total Loss after Epoch No: 9 - 1.0362467425755966 =&gt; Saving a new best Fooling rate: 99.598. Total Items Fooled :49799 Fooling rate: 99.486. Total Items Fooled :49743 Fooling rate: 99.456. Total Items Fooled :49728 Diversity Loss :0.9985426664352417 Fooling Loss: 0.008954382501542568 Total Loss after Epoch No: 10 - 1.038268227989857 =&gt; Saving a new best Fooling rate: 99.72. Total Items Fooled :49860 Fooling rate: 99.426. Total Items Fooled :49713 Fooling rate: 99.692. Total Items Fooled :49846 Diversity Loss :0.9976068735122681 Fooling Loss: 0.020556485280394554 Total Loss after Epoch No: 11 - 1.0406007697949042 =&gt; Saving a new best Fooling rate: 99.436. Total Items Fooled :49718 Fooling rate: 99.63. Total Items Fooled :49815 Fooling rate: 99.554. Total Items Fooled :49777 Diversity Loss :0.9977318048477173 Fooling Loss: 0.04298632964491844 Total Loss after Epoch No: 12 - 1.0370476129345405 =&gt; Saving a new best Fooling rate: 99.498. Total Items Fooled :49749 Fooling rate: 99.562. Total Items Fooled :49781 Fooling rate: 99.458. Total Items Fooled :49729 Diversity Loss :0.9988154172897339 Fooling Loss: 0.04159717634320259 Total Loss after Epoch No: 13 - 1.037070428713774 =&gt; Saving a new best Fooling rate: 99.368. Total Items Fooled :49684 Fooling rate: 99.644. Total Items Fooled :49822 Fooling rate: 99.446. Total Items Fooled :49723 Diversity Loss :0.9994094371795654 Fooling Loss: 0.0624578632414341 Total Loss after Epoch No: 14 - 1.0377162058766072 =&gt; Saving a new best Fooling rate: 99.696. Total Items Fooled :49848 Fooling rate: 99.788. Total Items Fooled :49894 Fooling rate: 99.494. Total Items Fooled :49747 Diversity Loss :0.9997531175613403 Fooling Loss: 0.035558152943849564 Total Loss after Epoch No: 15 - 1.0360386572205103 =&gt; Saving a new best Fooling rate: 99.678. Total Items Fooled :49839 Fooling rate: 99.574. Total Items Fooled :49787 Fooling rate: 99.548. Total Items Fooled :49774 Diversity Loss :0.9999127388000488 Fooling Loss: 0.07229295372962952 Total Loss after Epoch No: 16 - 1.041024495011721 =&gt; Saving a new best Fooling rate: 99.7. Total Items Fooled :49850 Fooling rate: 99.542. Total Items Fooled :49771 Fooling rate: 99.654. Total Items Fooled :49827 Diversity Loss :0.9986363649368286 Fooling Loss: 0.0533723421394825 Total Loss after Epoch No: 17 - 1.0388616713193746 =&gt; Saving a new best Fooling rate: 99.634. Total Items Fooled :49817 Fooling rate: 99.78. Total Items Fooled :49890 Fooling rate: 99.24. Total Items Fooled :49620 Diversity Loss :0.9978522658348083 Fooling Loss: 0.03446746990084648 Total Loss after Epoch No: 18 - 1.0345487464696934 =&gt; Saving a new best Fooling rate: 99.37. Total Items Fooled :49685 Fooling rate: 99.69. Total Items Fooled :49845 Fooling rate: 99.492. Total Items Fooled :49746 Diversity Loss :0.9980953335762024 Fooling Loss: 0.04048139601945877 Total Loss after Epoch No: 19 - 1.0351752294943883 =&gt; Saving a new best Fooling rate: 99.674. Total Items Fooled :49837 Fooling rate: 99.714. Total Items Fooled :49857 Fooling rate: 99.434. Total Items Fooled :49717 Diversity Loss :0.9989607334136963 Fooling Loss: 0.021351832896471024 Total Loss after Epoch No: 20 - 1.0392777105936637 =&gt; Saving a new best . Misc: . Setup Caffenet and VGG-F (TODO) . Paper : http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf | . Run the below code first before loading VGG-F . !{sys.executable} PrepareCaffenetModel.py . Loading VGG-F . #collapse-hide import torch from vgg import VGG_F model = vgg_f() model.load_state_dict(torch.load(&#39;VGG_FACE.caffemodel.pt&#39;)) model_dict[&#39;vgg-f&#39;] = model model(torch.rand((3,224,224))) . . Downloading Trained Weights . Pretrained Generator Weigths for Googlenet, Resnet50, VGG16 and VGG19 Avalaible as a Kaggle Dataset | Link : https://www.kaggle.com/gokkulnath/nag-pytorch-pretrained | . # Uncomment the below line after setting up kaggle api key # !kaggle datasets download -d gokkulnath/nag-pytorch-pretrained . Evaluating NAG performance across Models: (TODO) . For Tabular Column Generation | . Steps to evaluate the perturbations generated by Generator Network (TODO) . arch=&#39;Fixed&#39; for modelarch, model in model_dict.items(): num_iteration = 10 # Blackbox Settings if modelarch == arch: num_iteration =100 # Whitebox Settings for i range(num_iteration) 1. Load the Weights of the Generator 2. Generate a Perturbation using a random vector of dimension latent_dim,1 3. Add the noise to a sample image . Interpolating Latent Dimension for NAG . . Obtained Perturbations . References: . Official Code Repo : https://github.com/val-iisc/nag | GAN Architecture : Pytorch Tutorial | Transpose Convolution Docs | .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/adversarial%20machine%20learning/2020/03/30/Network-Adversary-Generation.html",
            "relUrl": "/adversarial%20machine%20learning/2020/03/30/Network-Adversary-Generation.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gokkulnath.github.io/adversarialwilderness/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am 23 years old. I was born in the city of Madurai in India. I lived in Coimbatore for four years while I was pursuing my undergraduate education.I have also lived in Trichy, Vellore and Chennai during school days. Currently I stay in Bengaluru, Karnataka. I completed my undergraduate education in Electronics and Communication Engineering from the Amrita School of Engineering, Coimbatore on May 2017. . I currently work as a Software Test Engineer, Testing Virtual Network Functions(VNF) and solve Cloud based challenges related to Telecom Industry at Ericsson, Bangalore, Karnataka. . I am interested in Deep Learning and Computer Vision with specific focus on GANs, Adversarial Machine learning and Federated Learning. I am inclined towards working on projects that involve working on the intersection of these fields. I enjoy playing Table Tennis, Fixing Broken Electronics, Watching Movies and Programming. . LinkedIn   GitHub   GitHub   GitHub .",
          "url": "https://gokkulnath.github.io/adversarialwilderness/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://gokkulnath.github.io/adversarialwilderness/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}