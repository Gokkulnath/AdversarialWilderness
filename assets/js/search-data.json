{
  
    
        "post0": {
            "title": "Research Ideas",
            "content": "Research Ideas (Outdated: 2014) . Activity recognition using Smartphone sensor. | Behavioural Diagnostics using Wireless Sensor Nodes.(Low Cost Scratch Built Node: Maniacbug) | Walk Style Monitoring and Categorizing based on IMU Sensor. | Micro Controller based Highly sensitive/precise power source for Critical Application (Programmable IC) | Use MicroUSB 3.0 OTG based hardware: Derive Power from Phone Battery: Develop Interface to Directly Connect Biomed Hardware and Process data using the Usb Interface. | Contactless ECG based on Micro Controller and Smartphone Realtime Signal Monitoring. | Emotion/Mood Detection ?? | ArduSpectro: Affordable spectrophotometry remains a challenge in the developing world and for mobile diagnostic teams in a domestic disaster response. | By using novel digital signal algorithms and off the shelf electronic components, the MIT team led by Dr. Paulino Vacas Jacques has created a | low-cost spectrophotomer used for disease and environmental marker detection | Smart Rectifier: Circuit Level Design to Optimise Power loss : SMPS/ Mobile Chargers | Microcotroller based Electric Energy Meters : Reduce Power Theft and Accurate Measurement of Power. | Any Machine Learning algorithm deployed on Embedded Controller. | Road Traffic Decongestion / Traffic optimisation using Clustering/Machine Learning approach. | Vechicle Collision avoidance using Ultrasonic Sensor Network based System . | Lifi/Bio Metric Based- Capacitive Sensor Based Security System | Speech Based Assistance to Blind People Using Deep Learning Architectures. | Adaptive Noise Cancellers/ Noise Supressers At Hospitals/Library. adaptive noise cancellation issues. echo cancellation |",
            "url": "https://gokkulnath.github.io/adversarialwilderness/ideas/2020/03/30/Research-Ideas-College.html",
            "relUrl": "/ideas/2020/03/30/Research-Ideas-College.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Network Adversary Generator",
            "content": "Introduction . Paper Abstract: . Adversarial perturbations can pose a serious threat for deploying machine learning systems. Recent works have shown existence of image-agnostic perturbations that can fool classifiers over most natural images. Existing methods present optimization approaches that solve for a fooling objective with an imperceptibility constraint to craft the perturbations making it very hard to defend. . Motivation . Current Approaches for crafting adversaries for a given classifier generate only one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. In order to build robust models, it is essential to explore diverse manifold of adversarial perturbations. This work can be of very useful, when we are using adversarial trainning, where the cost of generation of adversaries is high(Depends on the attack). With this approach, we will be able to generate adversarial noises from the learned distribution of adversarial perturbations. . Key Results: . The author&#39;s demonstrate that perturbations crafted by this model . achieve state-of-the-art fooling rates | exhibit wide variety | deliver excellent cross model generalizability. | Aproach . The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. The trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. . . Core idea is to model the distribution of universal adversarial perturbations for a given classifier. | The image shows a batch of B random vectors {z}B transforming into perturbations {delta}B by G which get added to the batch of data samples {x}B. | The top portion shows adversarial batch (XA), bottom portion shows shuffled adversarial batch (XS) and middle portion shows the benign batch (XB). The Fooling objective Lf and Diversity objective Ld constitute the loss. | Note: The target CNN (f) is a trained classifier and its parameters are not updated during the proposed training. On the other hand, the parameters of generator (G) are randomly initialized and learned through backpropagating the loss. (Best viewed in color). | . Note: Printable Versrion of the Entire Code disccused can be found Here: Link . Github Repo : https://github.com/Gokkulnath/NAG_Pytorch . Code: . Data Preparation . Download of Dataset P.S: Randomly Sampled 10 instances from each target class as described in the paper. | Option 1: Download from Archive.org Archive Link | train.zip | valid.zip | . | Option 2 : Mega Download Link for Train abd Validation data of Imagenet 2012 (Obtained from Kaggle) Validation Data: Mega Link | Trainning Data: Mega Link | . | Setting up of Folder Structure For Easier handling and reproducibility of results download from mega link | . Code Below Assumes Dataset is downlaoded and setup . Verify Dataset . #collapse-hide from glob import glob train_ok = True val_ok = True print(&quot;Training Data Verification&quot;) cls_count = len(glob(&quot;ILSVRC/train/*&quot;)) print(&quot;Total Number of Classes: {} in train directory&quot;.format(cls_count)) count = 0 for cls_ in glob(&quot;ILSVRC/train/*&quot;): imgs = glob(cls_ + &quot;/*&quot;) img_count = len(imgs) count += img_count if img_count != 10: print(cls_.split(&quot;/&quot;)[-1], img_count) train_ok=False print(&quot;Total {} number of files in {} classes. i.e 10 Images/Class&quot;.format(count, cls_count)) print(&quot;Validation Data Verification&quot;) val_files = glob(&quot;ILSVRC/valid/*&quot;) val_count = len(val_files) if val_count == 50000: print(&quot;Validation Data has correct number of files i.e {}&quot;.format(val_count)) else: print(&quot;Validation Data has some issue. Has following number of file : {}. Kindly Check!!&quot;.format(val_count)) val_ok=False if train_ok and val_ok: print(&quot;Dataset is Setup Correctly&quot;) . . Training Data Verification Total Number of Classes: 1000 in train directory Total 10000 number of files in 1000 classes. i.e 10 Images/Class Validation Data Verification Validation Data has correct number of files i.e 50000 Dataset is Setup Correctly . Imports . #collapse-hide import torch import torch.nn as nn from torch import optim import torch.nn.functional as F from torch.utils.data import DataLoader,Dataset import torchvision import torchvision.models as tvm from torchvision import transforms from torchvision.datasets.folder import DatasetFolder,ImageFolder import numpy as np from glob import glob from PIL import Image import pandas as pd import os,time,gc from pathlib import Path from tqdm import tqdm_notebook as tqdm import datetime,random,string . . ngpu=torch.cuda.device_count() device = torch.device(&quot;cuda&quot; if (torch.cuda.is_available() and ngpu &gt; 0) else &quot;cpu&quot;) print(&quot;Using Pytorch Version : {} and Torchvision Version : {}. Using Device {}&quot;.format(torch.__version__,torchvision.__version__,device)) . Using Pytorch Version : 1.4.0 and Torchvision Version : 0.5.0. Using Device cuda . Dataset and Dataloaders Setup . dataset_path=r&#39;ILSVRC/&#39; train_dataset_path=dataset_path+&#39;train&#39; test_dataset_path=dataset_path+&#39;valid&#39; print(&quot;Dataset root Folder:{}. Train Data Path: {}. Validation Data Path {}&quot;.format(dataset_path,train_dataset_path,test_dataset_path)) . Dataset root Folder:ILSVRC/. Train Data Path: ILSVRC/train. Validation Data Path ILSVRC/valid . # Preparation of Labels label_dict={} label_idx={} with open(&#39;ILSVRC/LOC_synset_mapping.txt&#39;) as file: lines=file.readlines() for idx,line in enumerate(lines): label,actual =line.strip(&#39; n&#39;).split(&#39; &#39;,maxsplit=1) label_dict[label]=actual label_idx[label]=idx . Transforms . # transforms size=224 # Imagenet Stats vgg_mean = [103.939, 116.779, 123.68] preprocess=transforms.Compose([transforms.Resize((size,size)), transforms.ToTensor(), transforms.Normalize(vgg_mean,(0.5, 0.5, 0.5))]) . Dataset and Dataloaders . class CustomDataset(Dataset): def __init__(self, subset, root_dir, transform=None): self.root_dir=root_dir self.transform=transform self.subset=subset if self.subset==&#39;train&#39;: data_dir=os.path.join(self.root_dir,self.subset) self.images_fn=glob(f&#39;{data_dir}/*/*&#39;) self.labels=[Path(fn).parent.name for fn in self.images_fn] elif subset ==&#39;valid&#39;: df=pd.read_csv(&#39;ILSVRC/LOC_val_solution.csv&#39;) df[&#39;label&#39;]=df[&#39;PredictionString&#39;].str.split(&#39; &#39;,n=1,expand=True)[0] df=df.drop(columns=[&#39;PredictionString&#39;]) self.images_fn=&#39;ILSVRC/valid/&#39;+df[&#39;ImageId&#39;].values+&#39;.JPEG&#39; self.labels=df[&#39;label&#39;] else: raise ValueError print(f&quot; Number of instances in {self.subset} subset of Dataset: {len(self.images_fn)}&quot;) def __getitem__(self,idx): fn=self.images_fn[idx] label=self.labels[idx] image=Image.open(fn) if image.getbands()[0] == &#39;L&#39;: image = image.convert(&#39;RGB&#39;) if self.transform: image = self.transform(image) return image,label_idx[label] def __len__(self): return len(self.images_fn) data_train=ImageFolder(root=&#39;ILSVRC/train&#39;,transform=preprocess) class2idx=data_train.class_to_idx data_valid=CustomDataset(subset=&#39;valid&#39;,root_dir=dataset_path,transform=preprocess) train_num = len(data_train) val_num = len(data_valid) . Number of instances in valid subset of Dataset: 50000 . Loss Functions/Objectives . def fooling_objective(qc_): &#39;&#39;&#39;Helper function to computer compute -log(1-qc&#39;), where qc&#39; is the adversarial probability of the class having maximum probability in the corresponding clean probability qc&#39; &gt; qc_ Parameters: prob_vec : Probability vector for the clean batch adv_prob_vec : Probability vecotr of the adversarial batch Returns: -log(1-qc&#39;) , qc&#39; &#39;&#39;&#39; # Get the largest probablities from predictions : Shape (bs,1) qc_=qc_.mean() return -1*torch.log(1-qc_) , qc_ def diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled): &#39;&#39;&#39;Helper function to calculate the cosine distance between two probability vectors Parameters: prob_vec : Probability vector for the clean batch adv_prob_vec : Probability vector for the adversarial batch Returns : Cosine distance between the corresponding clean and adversarial batches &#39;&#39;&#39; return torch.cosine_similarity(prob_vec_no_shuffle,prob_vec_shuffled).mean() ## TODO : Not Required. As we always take the last layer. def intermediate_activation_objective(layer_name=None): &#39;&#39;&#39; Extract the activations of any intermediate layer for: 1. batch of images (of batch size=32) corrupted by the perturbations (of batch size=32) 2. same batch of images corrupted by same batch of perturbations but in different (random) order (in this case the intermdeiate layer is set to &#39;res4f&#39; of ResNet 50 architecture) &#39;&#39;&#39; if arch ==&#39;resnet50&#39;: layer_name=&#39;res4f&#39; pass . Self Note: . Effect of ConvTranspose2d : It is a combination of upsampling and convolution layers used to increase the spatial resolution of the tensor | . Generator . Architecture of the generator (G) unchanged for different target CNN architectures | . . from torch import nn ngf=128 nz= latent_dim=10 e_lim = 10 nc=3 # Number of Channels # Fixed Architecture: Weights will be updated by Backprop. class AdveraryGenerator(nn.Module): def __init__(self,e_lim): super(AdveraryGenerator, self).__init__() self.e_lim = e_lim self.main = nn.Sequential( nn.ConvTranspose2d( in_channels=nz,out_channels= 1024, kernel_size=4, stride=1, padding=0, bias=False), nn.BatchNorm2d(1024), nn.ReLU(True), # state size. (ngf*8) x 4 x 4 nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True), # state size. (ngf*4) x 8 x 8 nn.ConvTranspose2d( 512, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True), # state size. (ngf*2) x 16 x 16 nn.ConvTranspose2d(256, 128, 4, 2, 2, bias=False), nn.BatchNorm2d(128), nn.ReLU(True), # state size. (ngf) x 32 x 32 nn.ConvTranspose2d( 128, 64, 4, 2, 2, bias=False), nn.BatchNorm2d(64), nn.ReLU(True), # state size. (nc) x 64 x 64 nn.ConvTranspose2d( 64, 3, 4, 4,4, bias=False), nn.BatchNorm2d(3), nn.ReLU(True), nn.Tanh() ) def forward(self, x): return self.e_lim * self.main(x) # Scaling of ε # move Generator to GPU if available adversarygen=AdveraryGenerator(e_lim).to(device) . Debugging . #collapse-hide if debug: try: from torchsummary import summary summary(adversarygen,(nz,1,1)) except: raise(&#39;Check torchsummary is installed. If not install using the command pip install torchsummary&#39;) . . Setting up Discriminator : Model : Architecture . from torchvision.models import googlenet, vgg16 , vgg19, resnet152, resnet50 model_dict ={ &#39;googlenet&#39;: googlenet, &#39;vgg16&#39;: vgg16 , &#39;vgg19&#39;:vgg19, &#39;resnet152&#39;:resnet152, # TODO Generate Perturbations &#39;resnet50&#39;:resnet50 # TODO Generate Perturbations } . Run only once : . #collapse-hide # Get all Pretrained Weights: for arch in model_dict.keys(): if arch !=&#39;vgg-f&#39;: model=model_dict[arch](pretrained=True) . . Choice of Hyperparameters . The architecture of the generator consists of 5 deconv layers. The final deconv layer is followed by a tanh non-linearity and scaling by epsillon (10) | . # epsillon=10 # batch_size=32 # latent_dim = 10 img_h,img_w,img_c=(224,224,3) latent_dim=10 arch=&#39;resnet50&#39; archs=model_dict.keys() # [&#39;vgg-f&#39;,&#39;vgg16&#39;,&#39;vgg19&#39;,&#39;googlenet&#39;,&#39;resnet50&#39;,&#39;resnet152&#39;] def get_bs(arch): if torch.cuda.is_available(): # GPU_BENCHMARK= 8192.0 # GPU_MAX_MEM = torch.cuda.get_device_properties(device).total_memory / (1024*1024) # BS_DIV= GPU_BENCHMARK/GPU_MAX_MEM # print(f&quot;Current GPU MAX Size : {GPU_MAX_MEM}. {BS_DIV}&quot;) if arch not in [&#39;resnet50&#39;,&#39;resnet152&#39;]:# [&#39;vgg16&#39;,&#39;vgg19&#39;,&#39;vgg-f&#39;,&#39;googlenet&#39;]: bs=int(64) elif arch in [&#39;resnet50&#39;,&#39;resnet152&#39;]: bs=int(32) else: raise ValueError(f&#39;Architecture type not supported. Please choose one from the following {archs}&#39;) else: bs=8 # OOM Error return bs get_bs(arch) . 32 . model=model_dict[arch](pretrained=True) model . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (avgpool): AdaptiveAvgPool2d(output_size=(1, 1)) (fc): Linear(in_features=2048, out_features=1000, bias=True) ) . Other Utils . #collapse-hide def save_checkpoint(model, to_save, filename=&#39;checkpoint.pth&#39;): &quot;&quot;&quot;Save checkpoint if a new best is achieved&quot;&quot;&quot; if to_save: print (&quot;=&gt; Saving a new best&quot;) torch.save(model.state_dict(), filename) # save checkpoint else: print (&quot;=&gt; Validation Accuracy did not improve&quot;) def save_perturbations(noise,arch,epoch,wabdb_flag=False): rand_str= &#39;&#39;.join( random.choice(string.ascii_letters) for i in range(6)) os.makedirs(f&quot;{arch}-{rand_str}&quot;,exist_ok=True) perturbations=noise.permute(0,2,3,1).cpu().detach().numpy()*255 np.save(f&#39;{arch}-{rand_str}/Perturbations_{arch}_{epoch}.npy&#39;, perturbations) for perturb_idx,perturbation in enumerate(perturbations[:,]): im = Image.fromarray(perturbation.astype(np.uint8)) if wabdb_flag: wandb.log({&quot;noise&quot;: [wandb.Image(im, caption=f&quot;Noise_{arch}_{epoch}_{perturb_idx}&quot;)]}) im.save(f&#39;{arch}-{rand_str}/Perturbations_{arch}_{epoch}_{perturb_idx}.png&#39;) # TODO def visualize_perturbations(): # MAtplotlib Subplot ? # Subplots(4*4) or (3*3) # From Memory or Disk - Epoch number ? pass def get_preds(predictions,return_idx=False, k=1): idxs= torch.argsort(predictions,descending=True)[:,:k] if return_idx: return predictions[:,idxs], idxs return predictions[:,idxs] . . Validating Model Utils . #collapse-hide # val_iterations = val_num/bs def compute_fooling_rate(prob_adv,prob_real): &#39;&#39;&#39;Helper function to calculate mismatches in the top index vector for clean and adversarial batch Parameters: prob_adv : Index vector for adversarial batch prob_real : Index vector for clean batch Returns: Number of mismatch and its percentage &#39;&#39;&#39; nfool=0 size = prob_real.shape[0] for i in range(size): if prob_real[i]!=prob_adv[i]: nfool = nfool+1 return nfool, 100*float(nfool)/size def validate_generator_old(noise,val_dl,val_iterations=10): total_fool=0 print(&quot;############### VALIDATION PHASE STARTED ################&quot;) train_log.writelines(&quot;############### VALIDATION PHASE STARTED ################&quot;) for val_idx in range(val_iterations): for batch_idx, data in enumerate(val_dl): images = data[0].to(device) # labels = data[1].to(device) prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q prob_vec_no_shuffle = D_model(images + noise) nfool, _ = compute_fooling_rate(prob_vec_no_shuffle,prob_vec_clean) total_fool += nfool fool_rate = 100*float(total_fool)/(val_iterations*batch_size) print(f&quot;Fooling rate: {foolr}. Total Items Fooled :{total_fool}&quot;) train_log.writelines(f&quot;Fooling rate: {foolr}. Total Items Fooled :{total_fool}&quot;) def validate_generator(noise,D_model,val_dl): total_fool=0 for batch_idx, data in tqdm(enumerate(val_dl),total = val_num//val_dl.batch_size): val_images = data[0].to(device) val_labels = data[1].to(device) prob_vec_clean,clean_idx = get_preds(F.softmax(D_model(val_images),dim=0),return_idx=True) # Variable q prob_vec_no_shuffle,adv_idx = get_preds(F.softmax(D_model(val_images + noise),dim=0),return_idx=True) nfool, _ = compute_fooling_rate(adv_idx,clean_idx) total_fool += nfool fool_rate = 100*float(total_fool)/(val_num) return fool_rate,total_fool . . Setup Wandb . Fit and Train the Generator . def fit(nb_epochs,D_model,dls,optimizer,adversarygen=adversarygen): # Set the Discriminator in Eval mode; Weights are fixed. train_dl,val_dl = dls D_model=D_model.to(device) D_model.eval() timestamp=datetime.datetime.now().strftime(&quot;%d%b%Y_%H_%M&quot;) train_log = open(f&#39;train_log_{arch}_{timestamp}.txt&#39;,&#39;w&#39;) for epoch in tqdm(range(nb_epochs),total=nb_epochs): running_loss=0 rand_str= &#39;&#39;.join( random.choice(string.ascii_letters) for i in range(6)) train_log.writelines(f&quot;############### TRAIN PHASE STARTED : {epoch}################&quot;) for batch_idx, data in tqdm(enumerate(train_dl),total = train_num//train_dl.batch_size): # Move Data and Labels to device(GPU) images = data[0].to(device) labels = data[1].to(device) # Generate the Adversarial Noise from Uniform Distribution U[-1,1] latent_seed = 2 * torch.rand(bs, nz, 1, 1, device=device,requires_grad=True) -1 # (r1 - r2) * torch.rand(a, b) + r2 noise = adversarygen(latent_seed) optimizer.zero_grad() # XB = images #preds_XB = f(images) prob_vec_clean = F.softmax(D_model(images),dim=0) # Variable q clean_preds ,clean_idx = get_preds(prob_vec_clean,return_idx=True,k=1) #XA = images+noise #preds_XA = f(images + noise) prob_vec_no_shuffle = D_model(images + noise) qc_ = F.softmax(prob_vec_no_shuffle,dim=0).gather(1,clean_idx) # Variable q&#39;c # 1. fooling_objective: encourages G to generate perturbations that decrease confidence of benign predictions fool_obj, mean_qc_ = fooling_objective(qc_) # Perturbations are shuffled across the batch dimesion to improve diversity #XS = images+ noise[torch.randperm(bs)] prob_vec_shuffled = D_model(images + noise[torch.randperm(bs)]) # 2. encourages Generator to explore the space of perturbations and generate a diverse set of perturbations divesity_obj=diversity_objective(prob_vec_no_shuffle, prob_vec_shuffled) # Compute Total Loss total_loss = divesity_obj + fool_obj # Lets perform Backpropagation to compute Gradients and update the weights total_loss.backward() optimizer.step() # wandb Logging : Expensive : Logs Perturbation Images each iteration # perturbations=noise.permute(0,2,3,1).cpu().detach().numpy()*255 # for perturb_idx,perturbation in enumerate(perturbations[:,]): # im = Image.fromarray(perturbation.astype(np.uint8)) # wandb.log({&quot;noise&quot;: [wandb.Image(im, caption=f&quot;Noise_{arch}_{epoch}_{perturb_idx}&quot;)]}) wandb.log({&quot;fool_obj&quot;: fool_obj.item(), &quot;divesity_obj&quot;: divesity_obj.item(), &quot;total_loss&quot;:total_loss.item(), }) running_loss += total_loss.item() if batch_idx!=0 and batch_idx % 100 ==0 : train_log.writelines(f&quot;############### VALIDATION PHASE STARTED : {epoch}, Step : {int(batch_idx / 100)} ################&quot;) fool_rate,total_fool= validate_generator(noise,D_model,val_dl) print(f&quot;Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}&quot;) train_log.writelines(f&quot;Fooling rate: {fool_rate}. Total Items Fooled :{total_fool}&quot;) print(f&quot;Diversity Loss :{divesity_obj.item()} n Fooling Loss: {fool_obj.item()} n&quot;) print(f&quot;Total Loss after Epoch No: {epoch +1} - {running_loss/(train_num//train_dl.batch_size)}&quot;) train_log.writelines(f&quot;Loss after Epoch No: {epoch +1} is {running_loss/(train_num//train_dl.batch_size)}&quot;) # to_save can be any expression/condition that returns a bool save_checkpoint(adversarygen, to_save= True, filename=f&#39;GeneratorW_{arch}_{epoch}_{rand_str}.pth&#39;) if epoch % 1 == 0: # save_perturbations(noise,arch,epoch) save_perturbations(noise,arch,epoch,wabdb_flag=True) train_log.close() . Start Actual Trainning . total_epochs = 20 lr = 1e-3 # Setting up Dataloaders import time,gc arch=&#39;resnet50&#39; start= time.time() print(f&quot;Training Generator for Arch {arch}&quot;) model= model_dict[arch](pretrained=True) bs = get_bs(arch) print(bs) train_dl=DataLoader(data_train,batch_size=bs,shuffle=True,num_workers=4,pin_memory=True,drop_last=True) val_dl=DataLoader(data_valid,batch_size=bs,shuffle=True,num_workers=4,pin_memory=True,drop_last=True) dls = [train_dl,val_dl] optimizer = optim.Adam(adversarygen.parameters(), lr=lr) print(f&quot;Elsasped Time {time.time()-start} Seconds&quot;) . Training Generator for Arch resnet50 32 Elsasped Time 0.6291134357452393 Seconds . fit(nb_epochs=total_epochs,D_model=model,dls=dls,optimizer=optimizer) . /home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` /home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` del sys.path[0] /home/ubuntu/miniconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:45: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0 Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook` . Fooling rate: 99.126. Total Items Fooled :49563 Fooling rate: 99.506. Total Items Fooled :49753 Fooling rate: 99.364. Total Items Fooled :49682 Diversity Loss :0.9992280602455139 Fooling Loss: 0.03950555995106697 Total Loss after Epoch No: 1 - 1.0430664758269603 =&gt; Saving a new best Fooling rate: 99.332. Total Items Fooled :49666 Fooling rate: 99.486. Total Items Fooled :49743 Fooling rate: 98.67. Total Items Fooled :49335 Diversity Loss :0.9981168508529663 Fooling Loss: 0.02838512323796749 Total Loss after Epoch No: 2 - 1.039582596757473 =&gt; Saving a new best Fooling rate: 99.446. Total Items Fooled :49723 Fooling rate: 99.52. Total Items Fooled :49760 Fooling rate: 99.38. Total Items Fooled :49690 Diversity Loss :0.9990019798278809 Fooling Loss: 0.004721864592283964 Total Loss after Epoch No: 3 - 1.039885509854708 =&gt; Saving a new best Fooling rate: 99.356. Total Items Fooled :49678 Fooling rate: 99.658. Total Items Fooled :49829 Fooling rate: 99.662. Total Items Fooled :49831 Diversity Loss :0.999189555644989 Fooling Loss: 0.003563906066119671 Total Loss after Epoch No: 4 - 1.0383393155076566 =&gt; Saving a new best Fooling rate: 99.62. Total Items Fooled :49810 Fooling rate: 99.486. Total Items Fooled :49743 Fooling rate: 99.686. Total Items Fooled :49843 Diversity Loss :0.9990421533584595 Fooling Loss: 0.008808250539004803 Total Loss after Epoch No: 5 - 1.0367735035908527 =&gt; Saving a new best Fooling rate: 99.744. Total Items Fooled :49872 Fooling rate: 99.338. Total Items Fooled :49669 Fooling rate: 99.692. Total Items Fooled :49846 Diversity Loss :0.9993197917938232 Fooling Loss: 0.005549242720007896 Total Loss after Epoch No: 6 - 1.037304274737835 =&gt; Saving a new best Fooling rate: 99.494. Total Items Fooled :49747 Fooling rate: 99.422. Total Items Fooled :49711 Fooling rate: 99.454. Total Items Fooled :49727 Diversity Loss :0.9981052875518799 Fooling Loss: 0.04303847253322601 Total Loss after Epoch No: 7 - 1.038387955763401 =&gt; Saving a new best Fooling rate: 99.29. Total Items Fooled :49645 Fooling rate: 99.416. Total Items Fooled :49708 Fooling rate: 99.558. Total Items Fooled :49779 Diversity Loss :0.998153567314148 Fooling Loss: 6.139297056506621e-06 Total Loss after Epoch No: 8 - 1.0364860896116648 =&gt; Saving a new best Fooling rate: 99.59. Total Items Fooled :49795 Fooling rate: 99.78. Total Items Fooled :49890 Fooling rate: 99.444. Total Items Fooled :49722 Diversity Loss :0.998258113861084 Fooling Loss: 0.002013623248785734 Total Loss after Epoch No: 9 - 1.0362467425755966 =&gt; Saving a new best Fooling rate: 99.598. Total Items Fooled :49799 Fooling rate: 99.486. Total Items Fooled :49743 Fooling rate: 99.456. Total Items Fooled :49728 Diversity Loss :0.9985426664352417 Fooling Loss: 0.008954382501542568 Total Loss after Epoch No: 10 - 1.038268227989857 =&gt; Saving a new best Fooling rate: 99.72. Total Items Fooled :49860 Fooling rate: 99.426. Total Items Fooled :49713 Fooling rate: 99.692. Total Items Fooled :49846 Diversity Loss :0.9976068735122681 Fooling Loss: 0.020556485280394554 Total Loss after Epoch No: 11 - 1.0406007697949042 =&gt; Saving a new best Fooling rate: 99.436. Total Items Fooled :49718 Fooling rate: 99.63. Total Items Fooled :49815 Fooling rate: 99.554. Total Items Fooled :49777 Diversity Loss :0.9977318048477173 Fooling Loss: 0.04298632964491844 Total Loss after Epoch No: 12 - 1.0370476129345405 =&gt; Saving a new best Fooling rate: 99.498. Total Items Fooled :49749 Fooling rate: 99.562. Total Items Fooled :49781 Fooling rate: 99.458. Total Items Fooled :49729 Diversity Loss :0.9988154172897339 Fooling Loss: 0.04159717634320259 Total Loss after Epoch No: 13 - 1.037070428713774 =&gt; Saving a new best Fooling rate: 99.368. Total Items Fooled :49684 Fooling rate: 99.644. Total Items Fooled :49822 Fooling rate: 99.446. Total Items Fooled :49723 Diversity Loss :0.9994094371795654 Fooling Loss: 0.0624578632414341 Total Loss after Epoch No: 14 - 1.0377162058766072 =&gt; Saving a new best Fooling rate: 99.696. Total Items Fooled :49848 Fooling rate: 99.788. Total Items Fooled :49894 Fooling rate: 99.494. Total Items Fooled :49747 Diversity Loss :0.9997531175613403 Fooling Loss: 0.035558152943849564 Total Loss after Epoch No: 15 - 1.0360386572205103 =&gt; Saving a new best Fooling rate: 99.678. Total Items Fooled :49839 Fooling rate: 99.574. Total Items Fooled :49787 Fooling rate: 99.548. Total Items Fooled :49774 Diversity Loss :0.9999127388000488 Fooling Loss: 0.07229295372962952 Total Loss after Epoch No: 16 - 1.041024495011721 =&gt; Saving a new best Fooling rate: 99.7. Total Items Fooled :49850 Fooling rate: 99.542. Total Items Fooled :49771 Fooling rate: 99.654. Total Items Fooled :49827 Diversity Loss :0.9986363649368286 Fooling Loss: 0.0533723421394825 Total Loss after Epoch No: 17 - 1.0388616713193746 =&gt; Saving a new best Fooling rate: 99.634. Total Items Fooled :49817 Fooling rate: 99.78. Total Items Fooled :49890 Fooling rate: 99.24. Total Items Fooled :49620 Diversity Loss :0.9978522658348083 Fooling Loss: 0.03446746990084648 Total Loss after Epoch No: 18 - 1.0345487464696934 =&gt; Saving a new best Fooling rate: 99.37. Total Items Fooled :49685 Fooling rate: 99.69. Total Items Fooled :49845 Fooling rate: 99.492. Total Items Fooled :49746 Diversity Loss :0.9980953335762024 Fooling Loss: 0.04048139601945877 Total Loss after Epoch No: 19 - 1.0351752294943883 =&gt; Saving a new best Fooling rate: 99.674. Total Items Fooled :49837 Fooling rate: 99.714. Total Items Fooled :49857 Fooling rate: 99.434. Total Items Fooled :49717 Diversity Loss :0.9989607334136963 Fooling Loss: 0.021351832896471024 Total Loss after Epoch No: 20 - 1.0392777105936637 =&gt; Saving a new best . Misc: . Setup Caffenet and VGG-F (TODO) . Paper : http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf | . Run the below code first before loading VGG-F . !{sys.executable} PrepareCaffenetModel.py . Loading VGG-F . #collapse-hide import torch from vgg import VGG_F model = vgg_f() model.load_state_dict(torch.load(&#39;VGG_FACE.caffemodel.pt&#39;)) model_dict[&#39;vgg-f&#39;] = model model(torch.rand((3,224,224))) . . Downloading Trained Weights . Pretrained Generator Weigths for Googlenet, Resnet50, VGG16 and VGG19 Avalaible as a Kaggle Dataset | Link : https://www.kaggle.com/gokkulnath/nag-pytorch-pretrained | . # Uncomment the below line after setting up kaggle api key # !kaggle datasets download -d gokkulnath/nag-pytorch-pretrained . Evaluating NAG performance across Models: (TODO) . For Tabular Column Generation | . Steps to evaluate the perturbations generated by Generator Network (TODO) . arch=&#39;Fixed&#39; for modelarch, model in model_dict.items(): num_iteration = 10 # Blackbox Settings if modelarch == arch: num_iteration =100 # Whitebox Settings for i range(num_iteration) 1. Load the Weights of the Generator 2. Generate a Perturbation using a random vector of dimension latent_dim,1 3. Add the noise to a sample image . Interpolating Latent Dimension for NAG . . Obtained Perturbations . References: . Official Code Repo : https://github.com/val-iisc/nag | GAN Architecture : Pytorch Tutorial | Transpose Convolution Docs | .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/adversarial%20machine%20learning/2020/03/30/Network-Adversary-Generation.html",
            "relUrl": "/adversarial%20machine%20learning/2020/03/30/Network-Adversary-Generation.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Post 0: Adversarial Machine Learning Paper Reading Challenge",
            "content": "Paper Summary Series Road-map : . Top 30 Key Research papers to get the necessary fundamental papers that anyone who wants to perform neural network evaluations should read. The papers are split by topic and indicated which topics should be read before others. (List shared by Nicholas Carlini ) . Preliminary Papers . [ ] Evasion Attacks against Machine Learning at Test Time | [ ] Intriguing properties of neural networks | [ ] Explaining and Harnessing Adversarial Examples | . Attacks [requires Preliminary Papers] . [ ] The Limitations of Deep Learning in Adversarial Settings | [ ] DeepFool: a simple and accurate method to fool deep neural networks | [ ] Towards Evaluating the Robustness of Neural Networks | . Transferability [requires Preliminary Papers] . [ ] Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples | [ ] Delving into Transferable Adversarial Examples and Black-box Attacks | [ ] Universal adversarial perturbations | . Detecting Adversarial Examples [requires Attacks, Transferability] . [ ] On Detecting Adversarial Perturbations | [ ] Detecting Adversarial Samples from Artifacts | [ ] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods | . Restricted Threat Model Attacks [requires Attacks] . [ ] ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models | [ ] Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models | [ ] Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors | . Physical-World Attacks [reqires Attacks, Transferability] . [ ] Adversarial examples in the physical world | [ ] Synthesizing Robust Adversarial Examples | [ ] Robust Physical-World Attacks on Deep Learning Models | . Verification [requires Introduction] . [ ] Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks | [ ] On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models | . Defenses (2) [requires Detecting] . [ ] Towards Deep Learning Models Resistant to Adversarial Attacks | [ ] Certified Robustness to Adversarial Examples with Differential Privacy | . Attacks (2) [requires Defenses (2)] . [ ] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples | [ ] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks | . Defenses (3) [requires Attacks (2)] . [ ] Towards the first adversarially robust neural network model on MNIST | [ ] On Evaluating Adversarial Robustness | . Other Domains [requires Attacks] . [ ] Adversarial Attacks on Neural Network Policies | [ ] Audio Adversarial Examples: Targeted Attacks on Speech-to-Text | [ ] Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples | [ ] Adversarial examples for generative models | . . Visualized using Adversarial Machine Learning Reading List by Nicholas Carlini — Link . Resources . Key Researchers their affiliations: . Nicholas Carlini (Google Brain) | Anish Athalye (MIT) | Nicolas Papernot (Google Brain) | Wieland Brendel (University of Tubingen) | Jonas Rauber (University of Tubingen) | Dimitris Tsipras (MIT) | Ian Goodfellow (Google Brain) | Aleksander Madry (MIT) | Alexey Kurakin (Google Brain) | . Research Labs : . Bethge Lab : http://bethgelab.org/ | . Frameworks/Libraries: . cleverhans- Tensorflow | foolbox — Keras/Tensorflow/Pytorch | advertorch — Pytorch | .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/adversarial%20machine%20learning/2020/02/15/Post-0-Adversarial-Machine-Learning-Paper-Reading-Challenge.html",
            "relUrl": "/adversarial%20machine%20learning/2020/02/15/Post-0-Adversarial-Machine-Learning-Paper-Reading-Challenge.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://gokkulnath.github.io/adversarialwilderness/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "TorchVision: My First Pull Request",
            "content": "Feature/Bug Requested: . The classes attribute of EMNIST dataset does not take into account the split argument. From the original EMNIST dataset https://www.nist.gov/itl/products-and-services/emnist-dataset . . Understanding the Problem: . The Existing code doesn’t consider the splits parameter that is passed to the dataset. This is because the EMNIST Class inherits the Default MNIST Class and doesn’t set the classes attribute appropriately. . Solution? . I knew that overriding the classes attribute should do the job. From the details provided in the issue and from the link, I was able to create a dictionary that maps the splits into classes. I was a not confident with my solution initially, but then I tried to use it locally with my changes in place and gained confidence over my changes. Finally, I override the classes attribute to get the expected behaviour. . Key Learnings: . Always create a branch for developing a feature or fixing a bug. Working on Master branch of fork will be a mess as other features can get merger to master branch of Original repo and syncing will be difficult. | Dont be afraid to commit mistakes or submit dumb solutions. Not all solutions are great. Maintainers are there to help and will review the changes and suggest better approaches if any . | Don’t forget to lint the code before pushing a change. I had to submit a few times to get the linting part correct. | . I Know the changes are not much, but I feel thrilled that I have a PR merged in torchvision repo. Thanks for reading. I encourage you to try to contribute to any Open Source project. I can assure it will be a great learning experience .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/pytorch/2020/01/11/TorchVision-PR.html",
            "relUrl": "/pytorch/2020/01/11/TorchVision-PR.html",
            "date": " • Jan 11, 2020"
        }
        
    
  
    
  
    
        ,"post7": {
            "title": "Choosing Components For Personal Deep Learning Machine",
            "content": ". Hi Everyone! . As a Hobbyist, the cost of EC2 Instances for running an experiment has been a barrier in exploring and solving Deep Learning Problems. Reserved Instances were my initial playground as i was not familiar with cloud ecosystem. . Eventually, Spot instances became my alternative to run well structured experiments. But often times, it found it very difficult to setup and run experiments. The main problem comes when setting up the environment for backing up and restoring the data/progress. Thanks to Alex Ramos and Slav Ivanov for the Classic and 24X7 versions of the EC2 Spotter tool which came in handy when dealing with spot instances.( Try them out if you still use Spot Instances ) . After using AWS EC2 instances for a around 6 months, I realized that the long term cheaper alternative is to invest on a local machine. This allows me to gain more by having better control over the experiment and with similar or better performance. On detailed survey throughout the internet, I couldn’t find any difference of opinion regarding the local machine idea when it comes to long term usage. Hence, I started to research on choosing components for my local deep learning build. . Selection of components for Deep learning is a a huge puzzle that intrigues many beginners who try to get their build. It requires the user to have some basic knowledge to build a system that can meet the required performance for the cost involved. . This post tries to help the fellow readers to get started with selection of components and understand the parameters before choosing the product. . So! Lets get Started!! . First things First! You must finalize on the maximum number of GPU’s that you plan to have on the newly built system. If you’re an active machine learning researcher then you might probably want more GPUs. This can help you run more than one task in parallel and try different variations of model architectures, data normalization, hyper parameters etc.. in parallel. . My Recommendations: If you are a researcher/Student/Hobbyist Consider for a Dual GPU Build. If you plan to run huge models and participate in insane contests like ImageNet which require heavy computation, consider for a Multi GPU Build. . Once you have arrived at a conclusion on the type of build you can arrive at the number of PCIe lanes required: . Dual-GPU Build (Up to 2 GPU): 24 PCIe Lanes (Might Experience Performance Lag when using SSD that share PCIe lanes or when Both GPU) . | Multi-GPU Build (Up to 4 GPU’s): 40 to 44 PCIe Lanes . | Why PCIe lanes first?— In practice, there will be a bottleneck to keep data flowing to the GPU because of disk access operations and/or data augmentation. A GPU would require 16 PCIe lanes to work at its full capacity. . This post will address only about Dual-GPU System. There will be a follow up post about the Multi-GPU Build. . Dual-GPU Build . 1) Motherboard: . Once the PCI-e Lane requirement has been decided, We can now choose the Motherboard Chipset: . The below table gives you the no of PCI-e Lanes available with different Chipsets available: . . Comparison of PCI-e Lanes across different Chipsets (Mostly Intel Processor Based) . Note: **Ideally a GPU, to perform at its full capacity requires **16 PCI-e Lanes. . So, even though Chipsets like B150,B250, H110,H170,H270 support Intel processors, They are seldom used for deep learning builds since the number of PCIe lanes will not be enough for Deep learning applications. . Hence, chipsets that are commonly preferred are: . Z170 — Support both 6th/7th Gen Intel Processor. Usage of 7th Gen might require a BIOS Update. Z270 — Support both 6th/7th Gen Intel Processor. (Latest) Z370 — Supports 8th Gen Intel Processor. . P.S: Will update the post once i have enough details for AMD based Chipsets . Once you have decided on the chipset, Use PC Partpicker to select the motherboard : Link to select the motherboard of your choice. . Things to Keep in Mind: . Form Factor (i.e ATX, Micro ATX, EATX etc..) . | No of PCIe Slots ( Minimum 2 Slots) . | Maximum RAM Supported ( 64 GB Preferred) . | No of RAM Slots (Minimum 4 Slots) . | SSD and SATA Slots (if you is concerned) . | 2) Processors: . Through the selection of motherboards, We have narrowed down the choice of processor based on constraints like socket type, But the choice of CPU might further dependent on GPU. For Deep learning applications, As mentioned earlier, The CPU is responsible mainly for the data processing and communicating with GPU. Hence, The number of cores and threads per core is important if we want to parallelize all that data preparation. It is advised to choose a multi core system (Preferably 4 Cores)to handle these tasks. . Things to Keep in Mind: . Socket Type . | No of Cores . | Cost . | PS. Some processors may need the user to get their own Cooler Fan. Usually, Unboxed Processor doesn’t come with a cooler fan but allows the user to overclock. . Use PC Partpicker to select the Processor : Link . Memory or RAM: . . When working with large/big datasets we might need to have them in memory. Size of the RAM decide how much of dataset you can hold in memory. For Deep learning applications it is suggested to have a minimum of 16GB memory (Jeremy Howard Advises to get 32GB). Regarding the Clock, The higher the better. It ideally signifies the Speed — Access Time but a minimum of 2400 MHz is advised. . Always try to get more memory in a single stick as it will allow for further expansion in remaining slots.I have seen many people who get 4*8 GB RAM instead of 2*16 GB ending up using all 4 Slots and no room for upgrade just because they are bit cheap than the latter. . Storage: . The price of HDD is decreasing continuosly as SSD become more affordable and faster. . . Source: Storagenewsletter . Its always better to get a small size SSD and a large HDD. SSD’s are preffered to store and retrieve data that is actively used. On the other hand HDD should be used to store data that are to be used in future. . SSD — Datasets in use + OS (Costly! Min: 128 GB Recommended) . HDD — Misc User Data (Cheaper! Min: 2 TB Recommended 7200RPM) . GPU: . GPU’s are the heart of Deep learning Build. They decide the performance gain that you get during training of neural networks. As most of the computation involved in Deep Learning are Matrix operations, GPU outperforms conventional CPU by running the same as parallel operations. They have small computation units called cores that can have threads which enable them to run the matrix operations faster. The Memory bandwidth of the GPU also enables to operate on large batches of data. . Tim Dettmers has a great article on choosing a GPU for Deep Learning, which he regularly updates as new cards come on the market. Please check them out before choosing your GPU. . Couldn’t resist to repost from Tim Dettmers Post. His TL;DR advice for choosing GPU: . Best GPU overall (by a small margin): Titan Xp Cost efficient but expensive: GTX 1080 Ti, GTX 1070, GTX 1080 Cost efficient and cheap: GTX 1060 (6GB) I work with data sets &gt; 250GB: GTX Titan X (Maxwell), NVIDIA Titan X Pascal, or NVIDIA Titan Xp I have little money: GTX 1060 (6GB) I have almost no money: GTX 1050 Ti (4GB) I do Kaggle: GTX 1060 (6GB) for any “normal” competition, or GTX 1080 Ti for “deep learning competitions” I am a competitive computer vision researcher: NVIDIA Titan Xp; do not upgrade from existing Titan X (Pascal or Maxwell) I am a researcher: GTX 1080 Ti. In some cases, like natural language processing, a GTX 1070 or GTX 1080 might also be a solid choice — check the memory requirements of your current models I want to build a GPU cluster: This is really complicated, you can get some ideas here I started deep learning and I am serious about it: Start with a GTX 1060 (6GB). Depending of what area, you choose next (start-up, Kaggle, research, applied deep learning) sell your GTX 1060 and buy something more appropriate I want to try deep learning, but I am not serious about it: GTX 1050 Ti (4 or 2GB) . I strongly recommend a beginner to get a 1060 6gb (New/Used) if they are on a budget. If the budget can go up a bit then you can get a 1070ti (MSRP around 430 USD) that was released recently i.e.. OCT 26th which offers almost the same performance as 1080 but at a lower cost (Almost Same as 1070). Don’t buy a 1070 unless you have a strong reason to, instead get a 1070ti as it has a greater number of cores. If you have enough money, then get a 1080ti. (No Second Thoughts). Again, if you are very active in performing research consider buying 2 X 1070ti instead of 1 X 1080ti as it gives flexibility that’s was discussed earlier. . For readers wondering about different editions of GPU like Founder’s Edition, OC, FTW etc. . Here’s the Info that you need: . Difference between Editions: Fundamentally all of them have the same GPU processor inside them. The main difference would be variation in quality of the PCB and usually high-end models would have higher binned chips (Best Quality). . Difference between Brands: Brands build their custom PCB components and aesthetics like lighting, Multiple fans, water cooled or back plate. These are done in order to improve the performance on the reference boards by just keeping the reference design on the card and add custom coolers on it. The base clocks out of the box matters very little generally. . Water vs Air cooled GPU: — Nvidia lowers the clock rate on your GPU as it gets hot. I don’t know if there are set temperatures that trigger this, or if it’s just linear. Water cooling will keep your GPU running at top speed. . Again! please research through the different editions. I have heard that FTW to be the coolest one to get. (Silent and No heating issues) . PSU: . Once the Components are selected using PCpartpicker site, it will give a rough estimate of power usage. It’s always better to get a PSU, large (1.2 to 1.5 times estimated power) enough to handle the power. In case if you plan to add more GPU(Add 100 W per GPU) then consider buying a PSU such that it can handle that requirement too. Some PSU tend to generate noise hence research on different products based on reviews before buying. I have found that EVGA G2 series seems to be solid option to consider. . Note: Gold, Silver, Platinum described along with the product refers to the efficiency of the PSU (Heat Generation). It directly correlates to power savings. . Conclusion: . For buying the components, I strongly recommend the reader to keep an eye on r/buildapcsales and r/hardwareswap (In US) for deals and grab them instead of buying them at retail price. Open-Box components seem to be cheaper and should be considered when on stringent budget. Try to get the components in instalments instead of getting them all at once if you are not in urgent need. . Cheaper Alternative: Try to get an Open-Box Pre-Built System and modify the components as per your requirement. (For Lazy Folks!) . The information described are based on my research and understanding from multiple articles and build guides from the internet. If there are any errors, please kindly notify me so that I can fix them. Feel free to contact me or comment below if have any questions. Thanks for reading! . Comparison of No of PCI-e Lanes across 200+ Chipsets (https://www.pugetsystems.com/labs/articles/Z270-H270-Q270-Q250-B250—What-is-the-Difference-876/) . | Comparison of No of PCI-e Lanes across 100+ Chipsets ( https://www.pugetsystems.com/labs/articles/Z170-H170-H110-B170-Q150-Q170—What-is-the-Difference-635/) . | Fast.ai Discussion Forum(Might require a Signup) (http://forums.fast.ai/t/making-your-own-server/174/506) . | PC Part Picker: https://pcpartpicker.com . | Choosing GPU for Deep learning (http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/) . | The $1700 Deep Learning box (https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415) . | FAQ &amp; Miscellaneous : . CPU Lanes vs PCI-e Lanes: . PCIe lane denotes the maximum bandwidth that is available for graphics cards’ communication with the CPU. Having more lanes than you need won’t increase performance, you just don’t want to have so few that it starts restricting CPU/GPU intercommunication. Generally an x8 lane of PCIe 3.0 has more than enough bandwidth for any gaming card, so 16 lanes for dual cards or 24 lanes for triple cards is fine. In applications outside of gaming, such as when the GPU is being used to accelerate CPU computation for workstations and servers, there is a lot more communication between the CPU and GPU than in games, so 40 lanes might be helpful there. The X99 platform is derived from Intel’s server/workstation chips, so that’s why they have so many lanes. . Source: Link .",
            "url": "https://gokkulnath.github.io/adversarialwilderness/2017/11/21/Choosing-Components-for-Personal-Deep-Learning-Machine.html",
            "relUrl": "/2017/11/21/Choosing-Components-for-Personal-Deep-Learning-Machine.html",
            "date": " • Nov 21, 2017"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "About Me",
          "content": ". I am 23 years old. I was born in the city of Madurai in India. I lived in Coimbatore for four years while I was pursuing my undergraduate education.I have also lived in Trichy, Vellore and Chennai during school days. Currently I stay in Bengaluru, Karnataka. I completed my undergraduate education in Electronics and Communication Engineering from the Amrita School of Engineering, Coimbatore on May 2017. . I currently work as a Software Test Engineer, Testing Virtual Network Functions(VNF) and solve Cloud based challenges related to Telecom Industry at Ericsson, Bangalore, Karnataka. . I am interested in Deep Learning and Computer Vision with specific focus on GANs, Adversarial Machine learning and Federated Learning. I am inclined towards working on projects that involve working on the intersection of these fields. I enjoy playing Table Tennis, Fixing Broken Electronics, Watching Movies and Programming. .       .",
          "url": "https://gokkulnath.github.io/adversarialwilderness/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page5": {
          "title": "News!",
          "content": "Activities! . External Internship Participant @Inkers.ai - EIP 2 and EIP 3 | AI Saturdays : Ambassador for Bangalore Chapter: Cycle 1 | Fast.AI international Fellow | One of the 15 Selected Outreach Participant to attend Computational Science Symposium (CSS-2017) | .",
          "url": "https://gokkulnath.github.io/adversarialwilderness/News/",
          "relUrl": "/News/",
          "date": ""
      }
      
  

  
      ,"page6": {
          "title": "Research Ideas",
          "content": "",
          "url": "https://gokkulnath.github.io/adversarialwilderness/research/",
          "relUrl": "/research/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

}