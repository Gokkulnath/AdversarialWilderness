{
  
    
        "post0": {
            "title": "Support Vector Machine(SVM)",
            "content": "Introduction . Support Vector Machine a.k.a SVM is one of the widely used classical machine learning technique since the late 1990&#39;s. This article tries to explain the internal workings of SVM and their usage using LIBSVM. . . Suport Vector Machine Theory . Let us assume a classical classification setting as shown in the image above. The solution to this problem setting is to find a decision boundary that can separate the data instances without any error and as accurate as possible. . The Core Idea of SVM is to find a ideal a hyperplane(Decision Boundary) which can best fit the given data points with a desired margin. SVM&#39;s construct two decision boundaries from the ideal margins to make the classification robust. As indicated in the below figure, the two decision boundary that separates the given data with a margin. These Margins are constructed in such a way that certain point/instances (a.k.a Support Vectors) are used such that the margins pass through them. . . Pros and Cons . Advantages: . Based on a strong and nice Theory: Has rigorous mathematical proof and validations | Training is relatively easy: No Local Optima Problem(Unlike NN) | Trainning Doesnt Depend on Dimesionality of Features but only on Inputs | | Generally avoids over-fitting: Classifier Compelxity and erros can be controlled using Hyperparams | Superior classification Accuracies: Generalize well when Data is Higher Dimension/High Cardinality Features and Under small training set conditions. &nbsp; | . | Disadvantages: . Selection of Kernel Function is Subjective | Identifying the Trade-off Hyperparameter C : Manual Search | Computationally Expensive in Multiclass problem settings. (Need to Train Multiple Classifiers) | . | . Notation and Concepts . HyperPlane : N-Dimensional Plane (Higher Dimensions(&gt;3) cannot be visualised) | Decision Boundary: A Hyper plane that partitions the data into sets with each set mapping to a class. | . Hyper Parameters . Kernel: . | Reglarisation Parameter (C) : . | Parameter Gamma | . Multi-Class vs One-Class SVM . LibSVM Usage . Setup : . Download the zip file using the url : link and unzip the archive. Copy the contents of Python Folder into the projects master/root directory . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . Example Use Case: . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . import sklearn from sklearn.datasets import load_breast_cancer . load_breast_cancer() . {&#39;data&#39;: array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01, 1.189e-01], [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01, 8.902e-02], [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01, 8.758e-02], ..., [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01, 7.820e-02], [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01, 1.240e-01], [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01, 7.039e-02]]), &#39;target&#39;: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), &#39;target_names&#39;: array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;), &#39;DESCR&#39;: &#39;.. _breast_cancer_dataset: n nBreast cancer wisconsin (diagnostic) dataset n-- n n**Data Set Characteristics:** n n :Number of Instances: 569 n n :Number of Attributes: 30 numeric, predictive attributes and the class n n :Attribute Information: n - radius (mean of distances from center to points on the perimeter) n - texture (standard deviation of gray-scale values) n - perimeter n - area n - smoothness (local variation in radius lengths) n - compactness (perimeter^2 / area - 1.0) n - concavity (severity of concave portions of the contour) n - concave points (number of concave portions of the contour) n - symmetry n - fractal dimension (&#34;coastline approximation&#34; - 1) n n The mean, standard error, and &#34;worst&#34; or largest (mean of the three n largest values) of these features were computed for each image, n resulting in 30 features. For instance, field 3 is Mean Radius, field n 13 is Radius SE, field 23 is Worst Radius. n n - class: n - WDBC-Malignant n - WDBC-Benign n n :Summary Statistics: n n ===================================== ====== ====== n Min Max n ===================================== ====== ====== n radius (mean): 6.981 28.11 n texture (mean): 9.71 39.28 n perimeter (mean): 43.79 188.5 n area (mean): 143.5 2501.0 n smoothness (mean): 0.053 0.163 n compactness (mean): 0.019 0.345 n concavity (mean): 0.0 0.427 n concave points (mean): 0.0 0.201 n symmetry (mean): 0.106 0.304 n fractal dimension (mean): 0.05 0.097 n radius (standard error): 0.112 2.873 n texture (standard error): 0.36 4.885 n perimeter (standard error): 0.757 21.98 n area (standard error): 6.802 542.2 n smoothness (standard error): 0.002 0.031 n compactness (standard error): 0.002 0.135 n concavity (standard error): 0.0 0.396 n concave points (standard error): 0.0 0.053 n symmetry (standard error): 0.008 0.079 n fractal dimension (standard error): 0.001 0.03 n radius (worst): 7.93 36.04 n texture (worst): 12.02 49.54 n perimeter (worst): 50.41 251.2 n area (worst): 185.2 4254.0 n smoothness (worst): 0.071 0.223 n compactness (worst): 0.027 1.058 n concavity (worst): 0.0 1.252 n concave points (worst): 0.0 0.291 n symmetry (worst): 0.156 0.664 n fractal dimension (worst): 0.055 0.208 n ===================================== ====== ====== n n :Missing Attribute Values: None n n :Class Distribution: 212 - Malignant, 357 - Benign n n :Creator: Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian n n :Donor: Nick Street n n :Date: November, 1995 n nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. nhttps://goo.gl/U2Uwz2 n nFeatures are computed from a digitized image of a fine needle naspirate (FNA) of a breast mass. They describe ncharacteristics of the cell nuclei present in the image. n nSeparating plane described above was obtained using nMultisurface Method-Tree (MSM-T) [K. P. Bennett, &#34;Decision Tree nConstruction Via Linear Programming.&#34; Proceedings of the 4th nMidwest Artificial Intelligence and Cognitive Science Society, npp. 97-101, 1992], a classification method which uses linear nprogramming to construct a decision tree. Relevant features nwere selected using an exhaustive search in the space of 1-4 nfeatures and 1-3 separating planes. n nThe actual linear program used to obtain the separating plane nin the 3-dimensional space is that described in: n[K. P. Bennett and O. L. Mangasarian: &#34;Robust Linear nProgramming Discrimination of Two Linearly Inseparable Sets&#34;, nOptimization Methods and Software 1, 1992, 23-34]. n nThis database is also available through the UW CS ftp server: n nftp ftp.cs.wisc.edu ncd math-prog/cpo-dataset/machine-learn/WDBC/ n n.. topic:: References n n - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction n for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on n Electronic Imaging: Science and Technology, volume 1905, pages 861-870, n San Jose, CA, 1993. n - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and n prognosis via linear programming. Operations Research, 43(4), pages 570-577, n July-August 1995. n - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques n to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) n 163-171.&#39;, &#39;feature_names&#39;: array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;), &#39;filename&#39;: &#39;C: Users Gokkulnath Anaconda3 envs infi lib site-packages sklearn datasets data breast_cancer.csv&#39;} . . References . You can display tables per the usual way in your blog: .",
            "url": "https://blog.gokkulnath.ml/machine%20learning/2021/02/09/_SVM-Quick-Walkthrough-using-LibSVM.html",
            "relUrl": "/machine%20learning/2021/02/09/_SVM-Quick-Walkthrough-using-LibSVM.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title : [ArxivID]-[ConfID]",
            "content": "Title [ConfID] . Authors : Auth Name 1, Auth Name 2 . Arxiv URL: Category: What type of paper is this? A measurement paper? An analysis of an existing system? A description of a research prototype? Context: Which other papers is it related to? Which theoretical bases were used to analyze the problem? Correctness: Do the assumptions appear to be valid? Contributions: What are the paper&#39;s main contributions? Clarity : Is the paper well written? . — . TLDR (100-200 Words) : . Key Motivations: . Problem Addressed or Why the Paper or the idea proposed ?? *Notation* (Optional) #### Propsed Method/Technique/Idea: #### New Concepts Introduced (Optional) . Previous Results: . Experiments Conducted: . Inferences from Table and Figures ![](/images/logo.png &quot;fast.ai&#39;s logo&quot;) . Key Conclusions . Possible Future Works/Extensions ? . .",
            "url": "https://blog.gokkulnath.ml/paperreview/%20adversarial-machine-learning/2020/07/20/Robustness-Union-models.html",
            "relUrl": "/paperreview/%20adversarial-machine-learning/2020/07/20/Robustness-Union-models.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Face Aging Using Cycle GAN",
            "content": "TLDR: . We Create 2 GAN with the objective to generate younger-older equivalents of old-young input images. . Old --&gt; Young : The Network has learned to remove wrinkles and add a lit bit of fairness to the face. . Young --&gt; Old : Tries to add wrinkles with a noisy patch pattern (Not perfect) and transforms eyebrow with appropriate aging effects. . CycleGAN Quick Intro: . Loss Objectives : . Adversarial Loss: Matching the Distribution of Generated Images to data distribution in the target domain. Similar to Normal GAN Loss, the idea is to use a discriminator network to classify the generated images as Fake or Real. In CycleGAN Case we have two discriminators which will evaluate the performance of Generators ability to model the target distribution. . Cycle Consistency Loss : Prevents the learned mappings of X and Y from contradicting each other. . Adversarial Loss alone can map the same set of input images to any random permutation of images in the target domain, where any of the . Identity Loss (Optional) : Use only when we need to preserved the color composition between input and output(eg. Paintings to Photos) It is achieved by Regularizing the generator near an idenity mapping when real images are fed into the generator. Prevents Altering of Tint of Input images . Results: . Analysis and Key Inferences . Limitations of CycleGAN . Tasks Which required Geometric Changes. (Can lead to Transfiguration) . Style Transfer vs CycleGAN : Style Transfer is setup to transfer styles from a single style image/instance and optimize accordingly, while CycleGAN can generate Stylized imgaes for multiple style images/instances . Horse to Zerbra: Humans also getting striped ? During trainning the model did not encounter images which have humans riding the horse or zebra .",
            "url": "https://blog.gokkulnath.ml/pytorch/project/2020/07/15/Face-AgingGan.html",
            "relUrl": "/pytorch/project/2020/07/15/Face-AgingGan.html",
            "date": " • Jul 15, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Support Vector Machine(SVM)",
            "content": "Introduction . Support Vector Machine a.k.a SVM is one of the widely used classical machine learning technique since the late 1990&#39;s. This article tries to explain the internal workings of SVM and their usage using LIBSVM. . . Suport Vector Machine Theory . Let us assume a classical classification setting as shown in the image above. The solution to this problem setting is to find a decision boundary that can separate the data instances without any error and as accurate as possible. . The Core Idea of SVM is to find a ideal a hyperplane(Decision Boundary) which can best fit the given data points with a desired margin. SVM&#39;s construct two decision boundaries from the ideal margins to make the classification robust. As indicated in the below figure, the two decision boundary that separates the given data with a margin. These Margins are constructed in such a way that certain point/instances (a.k.a Support Vectors) are used such that the margins pass through them. . . Pros and Cons . Advantages: . Based on a strong and nice Theory: Has rigorous mathematical proof and validations | Training is relatively easy: No Local Optima Problem(Unlike NN) | Trainning Doesnt Depend on Dimesionality of Features but only on Inputs | | Generally avoids over-fitting: Classifier Compelxity and erros can be controlled using Hyperparams | Superior classification Accuracies: Generalize well when Data is Higher Dimension/High Cardinality Features and Under small training set conditions. &nbsp; | . | Disadvantages: . Selection of Kernel Function is Subjective | Identifying the Trade-off Hyperparameter C : Manual Search | Computationally Expensive in Multiclass problem settings. (Need to Train Multiple Classifiers) | . | . Notation and Concepts . HyperPlane : N-Dimensional Plane (Higher Dimensions(&gt;3) cannot be visualised) | Decision Boundary: A Hyper plane that partitions the data into sets with each set mapping to a class. | . Hyper Parameters . Kernel: . | Reglarisation Parameter (C) : . | Parameter Gamma | . Multi-Class vs One-Class SVM . LibSVM Usage . Setup : . Download the zip file using the url : link and unzip the archive. Copy the contents of Python Folder into the projects master/root directory . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . Example Use Case: . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . import sklearn from sklearn.datasets import load_breast_cancer . load_breast_cancer() . {&#39;data&#39;: array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01, 1.189e-01], [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01, 8.902e-02], [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01, 8.758e-02], ..., [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01, 7.820e-02], [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01, 1.240e-01], [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01, 7.039e-02]]), &#39;target&#39;: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]), &#39;target_names&#39;: array([&#39;malignant&#39;, &#39;benign&#39;], dtype=&#39;&lt;U9&#39;), &#39;DESCR&#39;: &#39;.. _breast_cancer_dataset: n nBreast cancer wisconsin (diagnostic) dataset n-- n n**Data Set Characteristics:** n n :Number of Instances: 569 n n :Number of Attributes: 30 numeric, predictive attributes and the class n n :Attribute Information: n - radius (mean of distances from center to points on the perimeter) n - texture (standard deviation of gray-scale values) n - perimeter n - area n - smoothness (local variation in radius lengths) n - compactness (perimeter^2 / area - 1.0) n - concavity (severity of concave portions of the contour) n - concave points (number of concave portions of the contour) n - symmetry n - fractal dimension (&#34;coastline approximation&#34; - 1) n n The mean, standard error, and &#34;worst&#34; or largest (mean of the three n largest values) of these features were computed for each image, n resulting in 30 features. For instance, field 3 is Mean Radius, field n 13 is Radius SE, field 23 is Worst Radius. n n - class: n - WDBC-Malignant n - WDBC-Benign n n :Summary Statistics: n n ===================================== ====== ====== n Min Max n ===================================== ====== ====== n radius (mean): 6.981 28.11 n texture (mean): 9.71 39.28 n perimeter (mean): 43.79 188.5 n area (mean): 143.5 2501.0 n smoothness (mean): 0.053 0.163 n compactness (mean): 0.019 0.345 n concavity (mean): 0.0 0.427 n concave points (mean): 0.0 0.201 n symmetry (mean): 0.106 0.304 n fractal dimension (mean): 0.05 0.097 n radius (standard error): 0.112 2.873 n texture (standard error): 0.36 4.885 n perimeter (standard error): 0.757 21.98 n area (standard error): 6.802 542.2 n smoothness (standard error): 0.002 0.031 n compactness (standard error): 0.002 0.135 n concavity (standard error): 0.0 0.396 n concave points (standard error): 0.0 0.053 n symmetry (standard error): 0.008 0.079 n fractal dimension (standard error): 0.001 0.03 n radius (worst): 7.93 36.04 n texture (worst): 12.02 49.54 n perimeter (worst): 50.41 251.2 n area (worst): 185.2 4254.0 n smoothness (worst): 0.071 0.223 n compactness (worst): 0.027 1.058 n concavity (worst): 0.0 1.252 n concave points (worst): 0.0 0.291 n symmetry (worst): 0.156 0.664 n fractal dimension (worst): 0.055 0.208 n ===================================== ====== ====== n n :Missing Attribute Values: None n n :Class Distribution: 212 - Malignant, 357 - Benign n n :Creator: Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian n n :Donor: Nick Street n n :Date: November, 1995 n nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. nhttps://goo.gl/U2Uwz2 n nFeatures are computed from a digitized image of a fine needle naspirate (FNA) of a breast mass. They describe ncharacteristics of the cell nuclei present in the image. n nSeparating plane described above was obtained using nMultisurface Method-Tree (MSM-T) [K. P. Bennett, &#34;Decision Tree nConstruction Via Linear Programming.&#34; Proceedings of the 4th nMidwest Artificial Intelligence and Cognitive Science Society, npp. 97-101, 1992], a classification method which uses linear nprogramming to construct a decision tree. Relevant features nwere selected using an exhaustive search in the space of 1-4 nfeatures and 1-3 separating planes. n nThe actual linear program used to obtain the separating plane nin the 3-dimensional space is that described in: n[K. P. Bennett and O. L. Mangasarian: &#34;Robust Linear nProgramming Discrimination of Two Linearly Inseparable Sets&#34;, nOptimization Methods and Software 1, 1992, 23-34]. n nThis database is also available through the UW CS ftp server: n nftp ftp.cs.wisc.edu ncd math-prog/cpo-dataset/machine-learn/WDBC/ n n.. topic:: References n n - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction n for breast tumor diagnosis. IS&amp;T/SPIE 1993 International Symposium on n Electronic Imaging: Science and Technology, volume 1905, pages 861-870, n San Jose, CA, 1993. n - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and n prognosis via linear programming. Operations Research, 43(4), pages 570-577, n July-August 1995. n - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques n to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) n 163-171.&#39;, &#39;feature_names&#39;: array([&#39;mean radius&#39;, &#39;mean texture&#39;, &#39;mean perimeter&#39;, &#39;mean area&#39;, &#39;mean smoothness&#39;, &#39;mean compactness&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;, &#39;mean symmetry&#39;, &#39;mean fractal dimension&#39;, &#39;radius error&#39;, &#39;texture error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;, &#39;smoothness error&#39;, &#39;compactness error&#39;, &#39;concavity error&#39;, &#39;concave points error&#39;, &#39;symmetry error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst smoothness&#39;, &#39;worst compactness&#39;, &#39;worst concavity&#39;, &#39;worst concave points&#39;, &#39;worst symmetry&#39;, &#39;worst fractal dimension&#39;], dtype=&#39;&lt;U23&#39;), &#39;filename&#39;: &#39;C: Users Gokkulnath Anaconda3 envs infi lib site-packages sklearn datasets data breast_cancer.csv&#39;} . . References . You can display tables per the usual way in your blog: .",
            "url": "https://blog.gokkulnath.ml/machine%20learning/2020/05/24/_SVM-Quick-Walkthrough-using-LibSVM.html",
            "relUrl": "/machine%20learning/2020/05/24/_SVM-Quick-Walkthrough-using-LibSVM.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Research Ideas",
            "content": "Research Ideas (Outdated: 2015) . Activity recognition using Smartphone sensor. | Behavioural Diagnostics using Wireless Sensor Nodes.(Low Cost Scratch Built Node: Maniacbug) | Walk Style Monitoring and Categorizing based on IMU Sensor. | Micro Controller based Highly sensitive/precise power source for Critical Application (Programmable IC) | Use MicroUSB 3.0 OTG based hardware: Derive Power from Phone Battery: Develop Interface to Directly Connect Biomed Hardware and Process data using the Usb Interface. | Contactless ECG based on Micro Controller and Smartphone Realtime Signal Monitoring. | Emotion/Mood Detection ?? | ArduSpectro: Affordable spectrophotometry remains a challenge in the developing world and for mobile diagnostic teams in a domestic disaster response. | By using novel digital signal algorithms and off the shelf electronic components, the MIT team led by Dr. Paulino Vacas Jacques has created a | low-cost spectrophotomer used for disease and environmental marker detection | Smart Rectifier: Circuit Level Design to Optimise Power loss : SMPS/ Mobile Chargers | Microcotroller based Electric Energy Meters : Reduce Power Theft and Accurate Measurement of Power. | Any Machine Learning algorithm deployed on Embedded Controller. | Road Traffic Decongestion / Traffic optimisation using Clustering/Machine Learning approach. | Vechicle Collision avoidance using Ultrasonic Sensor Network based System . | Lifi/Bio Metric Based- Capacitive Sensor Based Security System | Speech Based Assistance to Blind People Using Deep Learning Architectures. | Adaptive Noise Cancellers/ Noise Supressers At Hospitals/Library. adaptive noise cancellation issues. echo cancellation |",
            "url": "https://blog.gokkulnath.ml/ideas/2020/03/30/Research-Ideas-College.html",
            "relUrl": "/ideas/2020/03/30/Research-Ideas-College.html",
            "date": " • Mar 30, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Network Adversary Generator",
            "content": "Introduction . TLDR . Adversarial perturbations can pose a serious threat for deploying machine learning systems. . Motivation . Current Approaches for crafting adversaries for a given classifier generate only one perturbation at a time, which is a single instance from the manifold of adversarial perturbations. In order to build robust models, it is essential to explore diverse manifold of adversarial perturbations. This work can be of very useful, when we are using adversarial trainning, where the cost of generation of adversaries is high(Depends on the attack). With this approach, we will be able to generate adversarial noises from the learned distribution of adversarial perturbations. . Key Results: . The author&#39;s demonstrate that perturbations crafted by this model . achieve state-of-the-art fooling rates | exhibit wide variety | deliver excellent cross model generalizability. | Aproach . The architecture of the proposed model is inspired from that of GANs and is trained using fooling and diversity objectives. The trained generator network attempts to capture the distribution of adversarial perturbations for a given classifier and readily generates a wide variety of such perturbations. . . Core idea is to model the distribution of universal adversarial perturbations for a given classifier. | The image shows a batch of B random vectors {z}B transforming into perturbations {delta}B by G which get added to the batch of data samples {x}B. | The top portion shows adversarial batch (XA), bottom portion shows shuffled adversarial batch (XS) and middle portion shows the benign batch (XB). The Fooling objective Lf and Diversity objective Ld constitute the loss. | Note: The target CNN (f) is a trained classifier and its parameters are not updated during the proposed training. On the other hand, the parameters of generator (G) are randomly initialized and learned through backpropagating the loss. (Best viewed in color). | . Note: Printable Version of the Entire Code disccused can be found Here: Link . Github Repo : https://github.com/Gokkulnath/NAG_Pytorch . Generator . Architecture of the generator (G): Model that is to be trained and remains unchanged for different target CNN architectures. | . . Choice of Hyperparameters . The architecture of the generator consists of 5 deconv layers. The final deconv layer is followed by a tanh non-linearity and scaling by epsillon (10) | . Setting up Discriminator : Model : Architecture . Validating Model and Metrics; Ablation Studies Discussion . Fooling Rate | . Pretrained Generator Weigths for Googlenet, Resnet50, VGG16 and VGG19 Avalaible as a Kaggle Dataset | Link : https://www.kaggle.com/gokkulnath/nag-pytorch-pretrained | . Reproducing the experiment: . Dataset and pretrained Generators cam be downloaded from this Kaggle Dataset or Execute the following line after setting up kaggle api key to get the dataset kaggle datasets download -d gokkulnath/nag-pytorch-pretrained . Results . Interpolating Latent Dimension for NAG . . Obtained Perturbations . References: . Official Code Repo : https://github.com/val-iisc/nag | GAN Architecture : Pytorch Tutorial | Transpose Convolution Docs | .",
            "url": "https://blog.gokkulnath.ml/adversarial-machine-learning/2020/03/29/NAG-Minimal.html",
            "relUrl": "/adversarial-machine-learning/2020/03/29/NAG-Minimal.html",
            "date": " • Mar 29, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Evasion attacks against machine learning at test time : [1708.06131]",
            "content": "Evasion attacks against machine learning at test time : [1708.06131] . Desired proactive protection mechanisms . finding potential vulnerabilities of learning before they are exploited by the adversary; | investigating the impact of the corresponding attacks (i.e., evaluating classifier security); | devising appropriate countermeasures if an attack is found to significantly degrade the classifier’s performance | General approach is to use Game-theory approach attacker vs defense till it reaches Nash equilibrium. but realsitic constaints are too hard to be incorporated into game theory . Adversary’s goal: . Adversary’s goal should be defined in terms of a utility (loss) function that the adversary seeks to maximize (minimize). | In the evasion setting, the attacker’s goal is to manipulate a single (without loss of generality, positive) sample that should be misclassified. | . Adversary Knowledge (Attackers Knowledge about the system,): . the training set or part of it; | the feature representation of each sample; i.e., how real objects such as emails, network packets are mapped into the classifier’s feature space; | the type of a learning algorithm and the form of its decision function; | the (trained) classifier model; e.g., weights of a linear classifier; | or feedback from the classifier; e.g., classifier labels for samples chosen by the adversary. | . Adversary’s capability.: . In the evasion scenario, the adversary’s capability is limited to modifications of test data; i.e.altering the training data is not allowed. However, under this restriction, variations in attacker’s power may include: . modifications to the input data (limited or unlimited); | modifications to the feature vectors (limited or unlimited); | or independent modifications to specific features (the semantics of the input data may dictate that certain features are interdependent). | . Attack Scenarios : . Perfect knowledge (PK) –&gt; The adversary knows the feature space, the type of the classifier, and the trained model.he adversary can transform attack points in the test data but must remain within a maximum distance of dmax from the original attack sample. Dmax constraint is added to make sure the semantic meaning from the real data is not lost. Limited knowledge (LK). The attacker knows the feature representation and the type of the classifier, but does not know either the learned classifier f or its training data D , and hence can not directly compute g(x). But the advesary has access to surrogate dataset D’ from the same underlying distribution as D. This can be done by sniffing some network traffic during the classifier operation, or by collecting legitimate samples from alternate source. Under this scenario , we try to approximate mimic the original classifier by trainning on the surrogate dataset with similar settings. Amount of Surrogate data is a attack hyper parameter . . well-known techniques, like gradient descent, or quadratic techniques such as Newton’s method, BFGS, or L-BFGS can be used to optimize this non linear optimization problem . when using gradient descent approach on non convex problems, we don’t have guarantee to arrive at global minima always. Hence at local minima the performance of the adversary will be poor and may not evade depending on the behavior of g(Approximated Function). To Overcome the effect/possiblity of local minima we add a λ* penalizer KDE (Kernel Density Estimator ) with bandwidth h. a.k.a (mimicry component.) The extra component favors attack points that imitate features of known legitimate samples. Which in turn reshapes the objective function and thereby biases the resulting gradient descent towards regions where the negative class is concentrated . Gradient descent attacks . TO DO : . Section 3 Redo : pseudo code to python translate . the gradient of kernel density estimators depends on the kernel gradient. .",
            "url": "https://blog.gokkulnath.ml/paperreview/%20adversarial%20machine%20learning/2020/02/16/1708.06131.html",
            "relUrl": "/paperreview/%20adversarial%20machine%20learning/2020/02/16/1708.06131.html",
            "date": " • Feb 16, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Explaining and Harnessing Adversarial Examples : [1412.6572]",
            "content": "Explaining and Harnessing Adversarial Examples : [1412.6572] . Intro . Explanation of Adversarial Examples . Understanding Norm . References: Wiki s . FGSM . Effect on Deep Networks and Weight decay . Explanation for Transferabilty of Perturbations . Experiments . Key Results .",
            "url": "https://blog.gokkulnath.ml/paperreview/%20adversarial%20machine%20learning/2020/02/16/1412.6572.html",
            "relUrl": "/paperreview/%20adversarial%20machine%20learning/2020/02/16/1412.6572.html",
            "date": " • Feb 16, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Post 0: Adversarial Machine Learning Paper Reading Challenge",
            "content": "Paper Summary Series Road-map : . Top 30 Key Research papers to get the necessary fundamental papers that anyone who wants to perform neural network evaluations should read. The papers are split by topic and indicated which topics should be read before others. (List shared by Nicholas Carlini ) . Preliminary Papers . [ ] Evasion Attacks against Machine Learning at Test Time | [ ] Intriguing properties of neural networks | [ ] Explaining and Harnessing Adversarial Examples | . Attacks [requires Preliminary Papers] . [ ] The Limitations of Deep Learning in Adversarial Settings | [ ] DeepFool: a simple and accurate method to fool deep neural networks | [ ] Towards Evaluating the Robustness of Neural Networks | . Transferability [requires Preliminary Papers] . [ ] Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples | [ ] Delving into Transferable Adversarial Examples and Black-box Attacks | [ ] Universal adversarial perturbations | . Detecting Adversarial Examples [requires Attacks, Transferability] . [ ] On Detecting Adversarial Perturbations | [ ] Detecting Adversarial Samples from Artifacts | [ ] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods | . Restricted Threat Model Attacks [requires Attacks] . [ ] ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models | [ ] Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models | [ ] Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors | . Physical-World Attacks [reqires Attacks, Transferability] . [ ] Adversarial examples in the physical world | [ ] Synthesizing Robust Adversarial Examples | [ ] Robust Physical-World Attacks on Deep Learning Models | . Verification [requires Introduction] . [ ] Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks | [ ] On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models | . Defenses (2) [requires Detecting] . [ ] Towards Deep Learning Models Resistant to Adversarial Attacks | [ ] Certified Robustness to Adversarial Examples with Differential Privacy | . Attacks (2) [requires Defenses (2)] . [ ] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples | [ ] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks | . Defenses (3) [requires Attacks (2)] . [ ] Towards the first adversarially robust neural network model on MNIST | [ ] On Evaluating Adversarial Robustness | . Other Domains [requires Attacks] . [ ] Adversarial Attacks on Neural Network Policies | [ ] Audio Adversarial Examples: Targeted Attacks on Speech-to-Text | [ ] Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples | [ ] Adversarial examples for generative models | . . Visualized using Adversarial Machine Learning Reading List by Nicholas Carlini — Link . Resources . Key Researchers their affiliations: . Nicholas Carlini (Google Brain) | Anish Athalye (MIT) | Nicolas Papernot (Google Brain) | Wieland Brendel (University of Tubingen) | Jonas Rauber (University of Tubingen) | Dimitris Tsipras (MIT) | Ian Goodfellow (Google Brain) | Aleksander Madry (MIT) | Alexey Kurakin (Google Brain) | . Research Labs : . Bethge Lab : http://bethgelab.org/ | . Frameworks/Libraries: . cleverhans- Tensorflow | foolbox — Keras/Tensorflow/Pytorch | advertorch — Pytorch | .",
            "url": "https://blog.gokkulnath.ml/adversarial-machine-learning/2020/02/15/Post-0-Adversarial-Machine-Learning-Paper-Reading-Challenge.html",
            "relUrl": "/adversarial-machine-learning/2020/02/15/Post-0-Adversarial-Machine-Learning-Paper-Reading-Challenge.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Adversarial Machine Learning Paper Reading Challenge",
            "content": "Paper Summary Series Road-map : . Top 30 Key Research papers to get the necessary fundamental papers that anyone who wants to perform neural network evaluations should read. The papers are split by topic and indicated which topics should be read before others. (List shared by Nicholas Carlini ) . Preliminary Papers . [ ] Evasion Attacks against Machine Learning at Test Time | [ ] Intriguing properties of neural networks | [ ] Explaining and Harnessing Adversarial Examples | . Attacks [requires Preliminary Papers] . [ ] The Limitations of Deep Learning in Adversarial Settings | [ ] DeepFool: a simple and accurate method to fool deep neural networks | [ ] Towards Evaluating the Robustness of Neural Networks | . Transferability [requires Preliminary Papers] . [ ] Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples | [ ] Delving into Transferable Adversarial Examples and Black-box Attacks | [ ] Universal adversarial perturbations | . Detecting Adversarial Examples [requires Attacks, Transferability] . [ ] On Detecting Adversarial Perturbations | [ ] Detecting Adversarial Samples from Artifacts | [ ] Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods | . Restricted Threat Model Attacks [requires Attacks] . [ ] ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models | [ ] Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models | [ ] Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors | . Physical-World Attacks [reqires Attacks, Transferability] . [ ] Adversarial examples in the physical world | [ ] Synthesizing Robust Adversarial Examples | [ ] Robust Physical-World Attacks on Deep Learning Models | . Verification [requires Introduction] . [ ] Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks | [ ] On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models | . Defenses (2) [requires Detecting] . [ ] Towards Deep Learning Models Resistant to Adversarial Attacks | [ ] Certified Robustness to Adversarial Examples with Differential Privacy | . Attacks (2) [requires Defenses (2)] . [ ] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples | [ ] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks | . Defenses (3) [requires Attacks (2)] . [ ] Towards the first adversarially robust neural network model on MNIST | [ ] On Evaluating Adversarial Robustness | . Other Domains [requires Attacks] . [ ] Adversarial Attacks on Neural Network Policies | [ ] Audio Adversarial Examples: Targeted Attacks on Speech-to-Text | [ ] Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples | [ ] Adversarial examples for generative models | . . Visualized using Adversarial Machine Learning Reading List by Nicholas Carlini — Link . Resources . Key Researchers their affiliations: . Nicholas Carlini (Google Brain) | Anish Athalye (MIT) | Nicolas Papernot (Google Brain) | Wieland Brendel (University of Tubingen) | Jonas Rauber (University of Tubingen) | Dimitris Tsipras (MIT) | Ian Goodfellow (Google Brain) | Aleksander Madry (MIT) | Alexey Kurakin (Google Brain) | . Research Labs : . Bethge Lab : http://bethgelab.org/ | . Frameworks/Libraries: . cleverhans- Tensorflow | foolbox — Keras/Tensorflow/Pytorch | advertorch — Pytorch | .",
            "url": "https://blog.gokkulnath.ml/adversarial-machine-learning/2020/02/15/Adversarial-Machine-Learning-Paper-Reading-Challenge.html",
            "relUrl": "/adversarial-machine-learning/2020/02/15/Adversarial-Machine-Learning-Paper-Reading-Challenge.html",
            "date": " • Feb 15, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "[a] fda feature disruptive attack",
            "content": "[A] FDA: Feature Disruptive Attack . Created: Aug 4, 2020 6:00 AM Status: 10 . Source: . Arxiv ID: 1909.04385 | URL: https://arxiv.org/abs/1909.04385 | . TLDR (100-200 Words) : . Generate image Specific perturbations by disrupting the inner feature representations at each layer of Neural networks. Existing attacks generate adversarial samples that optimize objective tied to softmax or pre-softmax layer of the network. Proposed method can construct stronger/better adversarial examples. FDA works by generating image perturbation that disrupt features at each layer of the network and causes deep-features to be highly corrupt. . Key Motivations: . Generate adversaries through optimization objectives that are tied to the pre-softmax or softmax output of the network. . In this work we, (i) show the drawbacks of such attacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack, and (iii) propose a new adversarial attack FDA: Feature Disruptive Attack, to address the drawbacks of existing attacks. . FDA works by generating image perturbation that disrupt features at each layer of the network and causes deep-features to be highly corrupt. . Problem Addressed or Why the Paper or the idea proposed ?? . Do deep features of adversarial samples retain usable clean sample information? . Existing attacks have optimization objectives ties to the softmax/pre-softmax layers, which retain high level of semantic information for its corresponding clean sample. (Validated using Feature inversion). | Inverted Features are same as clean sample. (Statistically validated) | Make model make predictions on adversarial samples that are semantically similar. (Observed Using New Metrics NLOR and OLNR) | . Proposed Soln : FDA generates perturbation with the aim to cause disruption of features at each layer of the network in a principled manner. . Advantages: FDA invariably flips the predicted label to highly unrelated classes, while also successfully removing evidence of the clean sample’s predicted label. . Can Work in Gray Box Setting (No Knowledge of Task/ Methodology) . Drawbacks : Image Specific Adversaries . Notation . y —&gt; (C Dimensional ) score vector Pre Softmax Scores . yGT —&gt; Ground Truth Label (Actual Class Argmax of softmax) . yML represents the class with the maximum predicted probability . Previous Results: . Sabour et al. specifically optimize to make a specific layer’s feature arbitrarily close to a target image’s features. Our objective is significantly different entailing disruption at every layer of a DNNs, without relying on a ’target’ image representation. | New Concepts Introduced . New evaluation metrics: Old Label New Rank (OLNR) and New Label Old Rank (NLOR) in order to quantify the extent of damage made by an attack. OLNR –&gt; Post the attack the change of rank from old label to new label | NLOR –&gt; Post the attack the change of rank of new label compared to old label | NRT-distance represents the average shift in the rank of the kth ordered statistic k. Primary benefit of NRT-distance measure is its robustness to outliers. | | Propose a new adversarial attack FDA: Feature Disruptive Attack, to address the drawbacks of existing attacks. | Adversarial samples: Input data containing imperceptible noise specifically crafted to manipulate the network’s prediction. . Fooling rate : Measures the strength of the attack ( percentage (%) of images for which the predicted label was changed due to the attack.) . Label leaking effect, is the phenomenon that during adversarial training, the validation error on adversarial examples are smaller than the validation error on clean examples. (Source http://jackhaha363.github.io/post/label_leak/) . TODO: . Feature inversion : Combining feature reconstruction with regularization objectives . Common Adversarial Defenses : Adversarial Trainning | Gradient Masking | Label Leaking | . | . Inferences from Table and Figures . Experiments Conducted: . Key Conclusions .",
            "url": "https://blog.gokkulnath.ml/2020/02/04/A-FDA-Feature-Disruptive-Attack.html",
            "relUrl": "/2020/02/04/A-FDA-Feature-Disruptive-Attack.html",
            "date": " • Feb 4, 2020"
        }
        
    
  
    
  
    
        ,"post12": {
            "title": "Membership Inference Attacks Against Machine Learning Models",
            "content": "",
            "url": "https://blog.gokkulnath.ml/paperreview/2020/01/20/MIA.html",
            "relUrl": "/paperreview/2020/01/20/MIA.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Ensemble Adversarial Trainning: Attacks and Defenses",
            "content": "",
            "url": "https://blog.gokkulnath.ml/paperreview/2020/01/20/EAT.html",
            "relUrl": "/paperreview/2020/01/20/EAT.html",
            "date": " • Jan 20, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.gokkulnath.ml/2020/01/14/test-markdown-post.html",
            "relUrl": "/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "TorchVision: My First Pull Request",
            "content": "Feature/Bug Requested: . The classes attribute of EMNIST dataset does not take into account the split argument. From the original EMNIST dataset https://www.nist.gov/itl/products-and-services/emnist-dataset . . Understanding the Problem: . The Existing code doesn’t consider the splits parameter that is passed to the dataset. This is because the EMNIST Class inherits the Default MNIST Class and doesn’t set the classes attribute appropriately. . Solution? . I knew that overriding the classes attribute should do the job. From the details provided in the issue and from the link, I was able to create a dictionary that maps the splits into classes. I was a not confident with my solution initially, but then I tried to use it locally with my changes in place and gained confidence over my changes. Finally, I override the classes attribute to get the expected behaviour. . Key Learnings: . Always create a branch for developing a feature or fixing a bug. Working on Master branch of fork will be a mess as other features can get merger to master branch of Original repo and syncing will be difficult. | Dont be afraid to commit mistakes or submit dumb solutions. Not all solutions are great. Maintainers are there to help and will review the changes and suggest better approaches if any . | Don’t forget to lint the code before pushing a change. I had to submit a few times to get the linting part correct. | . I Know the changes are not much, but I feel thrilled that I have a PR merged in torchvision repo. Thanks for reading. I encourage you to try to contribute to any Open Source project. I can assure it will be a great learning experience .",
            "url": "https://blog.gokkulnath.ml/pytorch/2020/01/11/TorchVision-PR.html",
            "relUrl": "/pytorch/2020/01/11/TorchVision-PR.html",
            "date": " • Jan 11, 2020"
        }
        
    
  
    
  
    
        ,"post17": {
            "title": "Torchsummary: model.summary() in PyTorch",
            "content": "Torchsummary: model.summary() in PyTorch . Repo: https://github.com/sksq96/pytorch-summary . Installation . pip install torch-summary | . Usage: from torchsummary import summary summary(your_model, input_size=(channels, H, W)) . Known Bugs . The code is written considering a gpu device is always available. It doesn’t work as such with cpu. You might encounter that model is torch.Tensor . ## .",
            "url": "https://blog.gokkulnath.ml/code%20walkthrough/pytorch/2019/12/30/torchsummary-Walkthrough.html",
            "relUrl": "/code%20walkthrough/pytorch/2019/12/30/torchsummary-Walkthrough.html",
            "date": " • Dec 30, 2019"
        }
        
    
  
    
        ,"post18": {
            "title": "CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
            "content": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks .",
            "url": "https://blog.gokkulnath.ml/paperreview/2019/11/25/1703.10593.html",
            "relUrl": "/paperreview/2019/11/25/1703.10593.html",
            "date": " • Nov 25, 2019"
        }
        
    
  
    
        ,"post19": {
            "title": "AIT: AI Assisted Triage",
            "content": "AIT: AI Assisted Triage . Motivation . Software industry has been evolving at rapid pace, with this evolution, the complexity of the systems is also on the rise. It is extremely challenging to ship quality software in complex, multi-component integrated system. To maintain quality and on time delivery of product we need to augment the existing system integration testing practices. . Using AI/ML we can optimize testing and predict failure points, thus reducing the overall cost and achieving high customer satisfaction. Analytics helps unlock the power of data and drives automation, improving QA efficiencies beyond the reach of traditional QA practices. . Mining the logs helps to conclude on identifying the defective component and can be used to trigger corrective action which would restore the services. This ensures the component functions as expected. . . Case Study: Challenges in QA/Software Testing . In complex multi-component telecom SIT environment, failure is a very common issue. To isolate and identify the component causing the issue is a tedious and time-consuming task. It involves meticulous troubleshooting into each component logs for possible problem identification, resolving it with corrective action plan and re-initiation of test. Further, it grows exponentially as the number of components interacting increases. . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.gokkulnath.ml/software-testing/2019/09/30/AIT.html",
            "relUrl": "/software-testing/2019/09/30/AIT.html",
            "date": " • Sep 30, 2019"
        }
        
    
  
    
        ,"post20": {
            "title": "REST API Client using Flask",
            "content": "REST vs SOAP 6 REST Principles Flask App : ? Python Anywhere Live ?? jcomgvac@rmailcloud.com username : jcomgvac email : koni@app-expert.com gokkulnath pythonanywhere host it live .",
            "url": "https://blog.gokkulnath.ml/python/2019/08/01/REST-API-Client-using-Flask.html",
            "relUrl": "/python/2019/08/01/REST-API-Client-using-Flask.html",
            "date": " • Aug 1, 2019"
        }
        
    
  
    
        ,"post21": {
            "title": "Parenting",
            "content": "My kid :The weekend Standup . ‌parenting has always been an unspoken topic especially among the Indian parents. Indian parents usually go with what the society dictates and often ask there children to blindly follow those guidelines. For eg.. Many children are forced to join engineering irrespective of their interests just because the society dictates that engineers are Great, well respected and well paid. So today, I propose a concept wherein parents spend some time span every week understanding their kids interests and needs. The idea is to discuss with the kid regarding the highlights of activities that they have done in the entire week. When the child shares their views, watch them very closely. If you find a situation that the child could have handled in a better way, ask them why and what was the rationale behind when they had to act for a particular situation, suggest them what you would have done and the reason behind it. This one activity will prepare your kid for the real world. Though they lack maturity to understand the political drama that the society revolves around they will get to know how to handle their immediately circle and react with emotional intelligence. Further, parents can also share their similar personal experience which can also enable the child to learn from parents experience. .",
            "url": "https://blog.gokkulnath.ml/2019/07/30/parenting.html",
            "relUrl": "/2019/07/30/parenting.html",
            "date": " • Jul 30, 2019"
        }
        
    
  
    
        ,"post22": {
            "title": "Object Detection Retail",
            "content": "Object Detection in Retail Shops - The challenges . Interest on Activity monitoring in retail shops has been in the rise since the announcement of Amazon Go. Last weekend I was at infliect a Computer vision based startup out of Bangalore which tries to solve the problem for a different reason. The reason being brands like PepsiCo for example need to understand how well the product sales are at all the retail outlets. They also need to ensure/validate that the retailers are displaying the products as per the service level agreement. That usually states that the shelf space at a retail shop must be more or less equal to the market share the brand has on the entire market. We had three very informative sessions from the team working on this problem at infliect. Confusion matrix on the bounding box labels was a new idea that was discussed. This way of inspecting the model gives us a better understanding of the failure cases. Unlike traditional metrics like mAP which gives only the quantitative aspect of error and fails to provide explanation on how the failures are associated among the bounding box .",
            "url": "https://blog.gokkulnath.ml/2019/07/30/Object-Detection-Retail.html",
            "relUrl": "/2019/07/30/Object-Detection-Retail.html",
            "date": " • Jul 30, 2019"
        }
        
    
  
    
        ,"post23": {
            "title": "Introduction to Callbacks and Implementing Learning Rate Annealing",
            "content": "Conventional Approach: Normal Training Loop . Issue with normal Training loops . TODO . Callback Approach . . Advantages Case Study : Mixed Precision Training : . Attaining Infinite Flexibility: . Callback Handler: . Keep Trainning Loop as Simple as possible Allows to write code for Each Idea/Tweak indepently in its own callback easy mix and match and perform ablation studies Examples: A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. Usually passed to the fit function callbacks parameters as a list.Callback : params: dict. Training parameters (eg. verbosity, batch size, number of epochs…) model: instance of keras.models.Model. Reference of the model being trained. Abstract Base Class Class Callback(object): “””Abstract base class used to build new callbacks.# Propertiesparams: dict. Training parameters(eg. verbosity, batch size, number of epochs…).model: instance of keras.models.Model.Reference of the model being trained.The logs dictionary that callback methodstake as argument will contain keys for quantities relevant tothe current batch or epoch.“”” 1. init → validation_data and model 2. set_params → params 3. set_model → Model 4. on_epoch_begin → Logs, epoch 5. on_epoch_end → Logs ,epoch 6. on_batch_begin → Logs, batch 7. on_batch_end → Logs, batch 8. on_train_begin → Logs 9. on_train_end → Logs Built in Callbacks in Keras : * TerminateOnNaN * BaseLogger * ProgbarLogger * History * ModelCheckpoint * EarlyStopping * RemoteMonitor * LearningRateScheduler * TensorBoard * ReduceLROnPlateau * CSVLogger * LambdaCallback Custom Callback : Custom callback can be created by extending the base class keras.callbacks.Callback. A callback has access to its associated model through the class property self.model. Callback : P.S: Difference Between Epoch and Batch Shared model and optim state Learning Rate Annealing One Cycle Learning Principle Reference : Keras Callbacks : https://keras.io/callbacks/ Cyclical Learning Rate (CLR) Github Code : https://github.com/bckenstler/CLR Sylvain Gugger’s Slides : Link https://drive.google.com/file/d/1eWWpyHeENyNNCVTtblX2Jm02WZWw-Kes/view .",
            "url": "https://blog.gokkulnath.ml/paperreview/2019/07/30/Callbacks-LRAnnealing.html",
            "relUrl": "/paperreview/2019/07/30/Callbacks-LRAnnealing.html",
            "date": " • Jul 30, 2019"
        }
        
    
  
    
        ,"post24": {
            "title": "Faster Neural Network Training with Data Echoing",
            "content": "Faster Neural Network Training with Data Echoing [1907.05550] . Main Idea: “data echoing,” which reduces the total computation used by earlier pipeline stages and speeds up training whenever computation upstream from accelerators dominates the training time. Data echoing reuses (or “echoes”) intermediate outputs from earlier pipeline stages in order to reclaim idle capacity. . upstream computation –&gt; Refers to Repetitive Computation towards the output . Observations : - Data echoing is a simple strategy for increasing hardware utilization when the training pipeline has a bottleneck in one of the upstream stages. - Echoing after augmentation was still effective at reducing the total number of examples read from disk, making it appealing for image datasets that employ expensive data augmentation that runs on the CPU. . Possible Concerns : Although a priori one might worry that SGD updates with repeated data would be useless or even harmful, for every workload we considered, at least one variant of data echoing reduced the total number of examples we needed to read from disk . Tradional Way : The training program first reads and decodes the input data, then shuffles the data, applies a set of transformations to augment the data, and gathers examples into batches. Finally, the program iteratively updates the neural network’s parameters to reduce its loss function over successive batches; we call this stage the “SGD update” regardless of which SGD variant is used. . Problem with Traditional Approach : Alternative orderings of these stages are also possible, and some pipelines might omit stages or add new ones. Since the output of any pipeline stage can be buffered, the computations in different stages overlap and the slowest stage dominates training time. Modern accelerators accentuate any slowness of other pipeline stages by making the optimized matrix operations less likely to dominate the total training time. . Idea : reuse the outputs of the first parts of the pipeline for multiple SGD updates to utilize idle processing capacity. echoing factor : number of times each intermediate output gets used . A data echoing algorithm inserts a repeat stage (optionally shuffling) somewhere in the training pipeline before the SGD update. Provided the time taken by upstream tasks (before the repeat stage) exceeds the time taken by downstream tasks (after the repeat stage), this technique will reclaim idle downstream compute capacity and increase the rate of SGD updates to the model . Advantages: . data echoing reduces the amount of upstream computation needed to reach a competitive out-of-sample error rate on various datasets and model architectures; | data echoing can support a wide range of echoing factors; | the effectiveness of data echoing depends on the insertion point in the training pipeline; | data echoing can benefit from additional shuffling after echoing, but does not require it; | countering expectations, data echoing reaches the same error rate as well-tuned baselines. | Difference Between Data echoing and Experience Replay: . data echoing and experience replay reuse previous data, our implementation of data echoing chooses the number of times to repeat each example, whereas most implementations of experience replay do not control this explicitly –&gt; “minibatch persistency” that reuses minibatches for multiple consecutive SGD updates : No +ve Results –&gt; “batch augmentation” that repeats examples multiple times within a given batch, but with different augmentations . Both of them claim to improve generalization, only tangentially mentioning the possibility of reclaiming idle computational capacity. .",
            "url": "https://blog.gokkulnath.ml/markdown/2019/07/17/1907.05550.html",
            "relUrl": "/markdown/2019/07/17/1907.05550.html",
            "date": " • Jul 17, 2019"
        }
        
    
  
    
        ,"post25": {
            "title": "Label Efficient Learning of Transferable Representations across Domains and Tasks [1712.00123]",
            "content": ". Idea .  Given a large labeled source dataset with annotations for a task set, A, we seek to transfer knowledge to a sparsely labeled target domain with a possibly wholly new task set, B. . Intuition . We should be able to learn reusable and general purpose representations which enable faster learning of future tasks requiring less human intervention . Key Concepts . Domain adaptation: Domain adaptation seeks to learn from related source domains a well  performing model on target data distribution. Existing work often assumes that both domains are defined on the same task and labeled data in target domain is sparse or non-existent [64]. Several methods have tackled the problem with the Maximum Mean Discrepancy (MMD) loss [17, 36, 37, 38, 73] between the source and target domain. Weight sharing of CNN parameters [58, 22, 21, 3] and  minimizing the distribution discrepancy of network activations [51, 65, 30] have also shown convincing results. Adversarial generative models [33, 32, 2, 59] aim at generating source-like data with target data by training a generator and a discriminator simultaneously, while adversarial discriminative models [62, 64, 13, 12, 23] focus on aligning embedding feature representations of target domain to source domain. Inspired by adversarial discriminative models, we propose a method that aligns domain features with multi-layer information. | Few-shot learning: | . Proposed Solution . Jointly adapt a source representation for use in a distinct target domain using a new multilayer unsupervised domain adversarial formulation while introducing a novel cross-domain and within domain class similarity objective. This new objective can be applied even when the target domain has non-overlapping classes to the source domain. . Evaluation . evaluate our approach in the challenging setting of joint transfer across domains and tasks and demonstrate our ability to successfully transfer, reducing the need for annotated data for the target domain and tasks.  Task : 1  -&gt; Transferring from a subset of SVHN containing only digits 0-4 to a subset of MNIST  containing only digits 5-9. Task : 2  -&gt; Adapting From ImageNet [6] object-centric images to UCF-101 [57] videos for action recognition. . Approach :  . Key Results: .",
            "url": "https://blog.gokkulnath.ml/paperreview/2019/05/30/1712.00123.html",
            "relUrl": "/paperreview/2019/05/30/1712.00123.html",
            "date": " • May 30, 2019"
        }
        
    
  
    
        ,"post26": {
            "title": "Understanding the Dynamics of Metrics",
            "content": "Understanding the Dynamics of Metrics . Spearman’s correlation coefficient Root Mean Squared Logarithmic Error (RMSLE) ROC AUC RMSE Normalized Weighted Root Mean Squared Logarithmic Error Normalized Gini Index Multiclass logarithmic loss (logloss) Mean Average Precision @ 5 (MAP@5) Matthews correlation coefficient (MCC) -Anomaly Detection, Classification Global Average Precision (GAP) - Multiclass Classification Dice coefficient Average Jaccard Index Accuracy is not the correct metric for this problem because it’s an imbalanced classification task! You want to evaluate based on the Receiver Operating Characteristic Area Under the Curve(ROC AUC). .",
            "url": "https://blog.gokkulnath.ml/paperreview/2019/03/22/metrics.html",
            "relUrl": "/paperreview/2019/03/22/metrics.html",
            "date": " • Mar 22, 2019"
        }
        
    
  
    
        ,"post27": {
            "title": " ",
            "content": "Explore and collect more info about * fully supervised transfer learning (FSTL), * weakly supervised learning (WSL), * semi-supervised learning (SSL) Transfer Learning Curriculum Learning : Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks TLDR ; Learning rate is always faster with curriculum learning, especially at the beginning of training. (ii) Final generalization is sometimes improved with curriculum learning, especially when the conditions for learning are hard: the task is difficult, the network is small, or - in a somewhat equivalent manner - when strong regularization is enforced . Progressive Resizing? Code Example : Fast.ai . Growing Dataset Grow the number of classes | Grow the number of Example of Classes: Few shot learning to Normal Learning | . |",
            "url": "https://blog.gokkulnath.ml/paperreview/2019/03/21/Leslie-Smith-Ideas.html",
            "relUrl": "/paperreview/2019/03/21/Leslie-Smith-Ideas.html",
            "date": " • Mar 21, 2019"
        }
        
    
  
    
        ,"post28": {
            "title": "Choosing Components For Personal Deep Learning Machine",
            "content": ". Hi Everyone! . As a Hobbyist, the cost of EC2 Instances for running an experiment has been a barrier in exploring and solving Deep Learning Problems. Reserved Instances were my initial playground as i was not familiar with cloud ecosystem. . Eventually, Spot instances became *my alternative to run well structured experiments. But often times, it found it very difficult to setup and run experiments. The main problem comes when setting up the environment for backing up and restoring the data/progress. Thanks to Alex Ramos and Slav Ivanov for the Classic and 24X7 versions of the EC2 Spotter tool which came in handy when dealing with spot instances.( Try them out if you still use Spot Instances* ) . After using AWS EC2 instances for a around 6 months, I realized that the long term cheaper alternative is to invest on a local machine. This allows me to gain more by having better control over the experiment and with similar or better performance. On detailed survey throughout the internet, I couldn’t find any difference of opinion regarding the local machine idea when it comes to long term usage. Hence, I started to research on choosing components for my local deep learning build. . Selection of components for Deep learning is a a huge puzzle that intrigues many beginners who try to get their build. It requires the user to have some basic knowledge to build a system that can meet the required performance for the cost involved. . This post tries to help the fellow readers to get started with selection of components and understand the parameters before choosing the product. . So! Lets get Started!! . First things First! You must finalize on the maximum number of GPU’s that you plan to have on the newly built system. If you’re an active machine learning researcher then you might probably want more GPUs. This can help you run more than one task in parallel and try different variations of model architectures, data normalization, hyper parameters etc.. in parallel. . My Recommendations: If you are a researcher/Student/Hobbyist Consider for a Dual GPU Build. If you plan to run huge models and participate in insane contests like ImageNet which require heavy computation, consider for a Multi GPU Build. . Once you have arrived at a conclusion on the type of build you can arrive at the number of PCIe lanes required: . Dual-GPU Build (Up to 2 GPU): 24 PCIe Lanes (Might Experience Performance Lag when using SSD that share PCIe lanes or when Both GPU) | Multi-GPU Build (Up to 4 GPU’s): 40 to 44 PCIe Lanes | Why PCIe lanes first?— In practice, there will be a bottleneck to keep data flowing to the GPU because of disk access operations and/or data augmentation. A GPU would require 16 PCIe lanes to work at its full capacity. . This post will address only about Dual-GPU System. There will be a follow up post about the Multi-GPU Build. . Dual-GPU Build . 1) Motherboard: . Once the PCI-e Lane requirement has been decided, We can now choose the Motherboard Chipset: . The below table gives you the no of PCI-e Lanes available with different Chipsets available: . . Comparison of PCI-e Lanes across different Chipsets (Mostly Intel Processor Based) . Note:** Ideally a GPU, to perform at its full capacity requires **16 PCI-e Lanes. . So, even though Chipsets like B150,B250, H110,H170,H270 support Intel processors, They are seldom used for deep learning builds since the number of PCIe lanes will not be enough for Deep learning applications. . Hence, chipsets that are commonly preferred are: . Z170 — Support both 6th/7th Gen Intel Processor. Usage of 7th Gen might require a BIOS Update. Z270 — Support both 6th/7th Gen Intel Processor. (Latest) Z370 — Supports 8th Gen Intel Processor. . P.S: Will update the post once i have enough details for AMD based Chipsets . Once you have decided on the chipset, Use PC Partpicker to select the motherboard : Link to select the motherboard of your choice. . Things to Keep in Mind: . Form Factor (i.e ATX, Micro ATX, EATX etc..) | No of PCIe Slots ( Minimum 2 Slots) | Maximum RAM Supported ( 64 GB Preferred) | No of RAM Slots (Minimum 4 Slots) | SSD and SATA Slots (if you is concerned) | 2) Processors: . Through the selection of motherboards, We have narrowed down the choice of processor based on constraints like socket type, But the choice of CPU might further dependent on GPU. For Deep learning applications, As mentioned earlier, The CPU is responsible mainly for the data processing and communicating with GPU. Hence, The number of cores and threads per core is important if we want to parallelize all that data preparation. It is advised to choose a multi core system (Preferably 4 Cores)to handle these tasks. . Things to Keep in Mind: . Socket Type | No of Cores | Cost | Some processors may need the user to get their own Cooler Fan. Usually, Unboxed Processor doesn’t come with a cooler fan but allows the user to overclock. | Use PC Partpicker to select the Processor : Link . Memory or RAM: . . When working with large/big datasets we might need to have them in memory. Size of the RAM decide how much of dataset you can hold in memory. For Deep learning applications it is suggested to have a minimum of 16GB memory (Jeremy Howard Advises to get 32GB). Regarding the Clock, The higher the better. It ideally signifies the Speed — Access Time but a minimum of 2400 MHz is advised. . Always try to get more memory in a single stick as it will allow for further expansion in remaining slots.I have seen many people who get 48 GB RAM instead of 216 GB ending up using all 4 Slots and no room for upgrade just because they are bit cheap than the latter. . Storage: . The price of HDD is decreasing continuously as SSD become more affordable and faster. . . Its always better to get a small size SSD and a large HDD. SSD’s are preferred to store and retrieve data that is actively used. On the other hand HDD should be used to store data that are to be used in future. . SSD — Datasets in use + OS (Costly! Min: 128 GB Recommended) . HDD — Misc User Data (Cheaper! Min: 2 TB Recommended 7200RPM) . GPU: . GPU’s are the heart of Deep learning Build. They decide the performance gain that you get during training of neural networks. As most of the computation involved in Deep Learning are Matrix operations, GPU outperforms conventional CPU by running the same as parallel operations. They have small computation units called cores that can have threads which enable them to run the matrix operations faster. The Memory bandwidth of the GPU also enables to operate on large batches of data. . Tim Dettmers has a great article on choosing a GPU for Deep Learning, which he regularly updates as new cards come on the market. Please check them out before choosing your GPU. . Couldn’t resist to repost from Tim Dettmers Post. His TL;DR advice for choosing GPU: . Best GPU overall (by a small margin): Titan Xp . Cost efficient but expensive: GTX 1080 Ti, GTX 1070, GTX 1080 . Cost efficient and cheap: GTX 1060 (6GB) . I work with data sets &gt; 250GB: GTX Titan X (Maxwell), NVIDIA Titan X Pascal, or NVIDIA Titan Xp . I have little money: GTX 1060 (6GB) . I have almost no money: GTX 1050 Ti (4GB) . I do Kaggle: GTX 1060 (6GB) for any “normal” competition, or GTX 1080 Ti for “deep learning competitions” . I am a competitive computer vision researcher: NVIDIA Titan Xp; do not upgrade from existing Titan X (Pascal or Maxwell) . I am a researcher: GTX 1080 Ti. In some cases, like natural language processing, a GTX 1070 or GTX 1080 might also be a solid choice — check the memory requirements of your current models . I want to build a GPU cluster: This is really complicated, you can get some ideas here . I started deep learning and I am serious about it: Start with a GTX 1060 (6GB). Depending of what area, you choose next (start-up, Kaggle, research, applied deep learning) sell your GTX 1060 and buy something more appropriate . I want to try deep learning, but I am not serious about it: GTX 1050 Ti (4 or 2GB) . I strongly recommend a beginner to get a 1060 6gb (New/Used) if they are on a budget. If the budget can go up a bit then you can get a 1070ti (MSRP around 430 USD) that was released recently i.e.. OCT 26th which offers almost the same performance as 1080 but at a lower cost (Almost Same as 1070). Don’t buy a 1070 unless you have a strong reason to, instead get a 1070ti as it has a greater number of cores. If you have enough money, then get a 1080ti. (No Second Thoughts). Again, if you are very active in performing research consider buying 2 X 1070ti instead of 1 X 1080ti as it gives flexibility that’s was discussed earlier. . For readers wondering about different editions of GPU like Founder’s Edition, OC, FTW etc. . Here’s the Info that you need: . Difference between Editions: Fundamentally all of them have the same GPU processor inside them. The main difference would be variation in quality of the PCB and usually high-end models would have higher binned chips (Best Quality). . Difference between Brands: Brands build their custom PCB components and aesthetics like lighting, Multiple fans, water cooled or back plate. These are done in order to improve the performance on the reference boards by just keeping the reference design on the card and add custom coolers on it. The base clocks out of the box matters very little generally. . Water vs Air cooled GPU: — Nvidia lowers the clock rate on your GPU as it gets hot. I don’t know if there are set temperatures that trigger this, or if it’s just linear. Water cooling will keep your GPU running at top speed. . Again! please research through the different editions. I have heard that FTW to be the coolest one to get. (Silent and No heating issues) . PSU: . Once the Components are selected using PCpartpicker site, it will give a rough estimate of power usage. It’s always better to get a PSU, large (1.2 to 1.5 times estimated power) enough to handle the power. In case if you plan to add more GPU(Add 100 W per GPU) then consider buying a PSU such that it can handle that requirement too. Some PSU tend to generate noise hence research on different products based on reviews before buying. I have found that EVGA G2 series seems to be solid option to consider. . Note: Gold, Silver, Platinum described along with the product refers to the efficiency of the PSU (Heat Generation). It directly correlates to power savings. . Conclusion: . For buying the components, I strongly recommend the reader to keep an eye on r/buildapcsales and r/hardwareswap (In US) for deals and grab them instead of buying them at retail price. Open-Box components seem to be cheaper and should be considered when on stringent budget. Try to get the components in instalments instead of getting them all at once if you are not in urgent need. . Cheaper Alternative: Try to get an Open-Box Pre-Built System and modify the components as per your requirement. (For Lazy Folks!) . The information described are based on my research and understanding from multiple articles and build guides from the internet. If there are any errors, please kindly notify me so that I can fix them. Feel free to contact me or comment below if have any questions. Thanks for reading! . Comparison of No of PCI-e Lanes across 200+ Chipsets (https://www.pugetsystems.com/labs/articles/Z270-H270-Q270-Q250-B250—What-is-the-Difference-876/) | Comparison of No of PCI-e Lanes across 100+ Chipsets ( https://www.pugetsystems.com/labs/articles/Z170-H170-H110-B170-Q150-Q170—What-is-the-Difference-635/) | ai Discussion Forum(Might require a Signup) (http://forums.fast.ai/t/making-your-own-server/174/506) | PC Part Picker: https://pcpartpicker.com | Choosing GPU for Deep learning (http://timdettmers.com/2017/04/09/which-gpu-for-deep-learning/) | The $1700 Deep Learning box (https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415) | FAQ &amp; Miscellaneous : . CPU Lanes vs PCI-e Lanes: . PCIe lane denotes the maximum bandwidth that is available for graphics cards’ communication with the CPU. Having more lanes than you need won’t increase performance, you just don’t want to have so few that it starts restricting CPU/GPU intercommunication. Generally an x8 lane of PCIe 3.0 has more than enough bandwidth for any gaming card, so 16 lanes for dual cards or 24 lanes for triple cards is fine. In applications outside of gaming, such as when the GPU is being used to accelerate CPU computation for workstations and servers, there is a lot more communication between the CPU and GPU than in games, so 40 lanes might be helpful there. The X99 platform is derived from Intel’s server/workstation chips, so that’s why they have so many lanes. . Source: Link .",
            "url": "https://blog.gokkulnath.ml/misc-advice/2017/11/21/Choosing-Components-for-Personal-Deep-Learning-Machine.html",
            "relUrl": "/misc-advice/2017/11/21/Choosing-Components-for-Personal-Deep-Learning-Machine.html",
            "date": " • Nov 21, 2017"
        }
        
    
  
    
        ,"post29": {
            "title": "A Novel Clustered Support Vector Machine with Reduced Support Vectors for Big Data Classification",
            "content": "If you are not redirected automatically, follow this link to get to know about the project. You can find the code related to the project at Code. .",
            "url": "https://blog.gokkulnath.ml/machine%20learning/2016/03/11/Clustered-SVM.html",
            "relUrl": "/machine%20learning/2016/03/11/Clustered-SVM.html",
            "date": " • Mar 11, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". I am 24 years old. I was born in the city of Madurai in India. I lived in Coimbatore for four years while I was pursuing my undergraduate education.I have also lived in Trichy, Vellore and Chennai during school days. Currently I stay in Bengaluru, Karnataka. I completed my undergraduate education in Electronics and Communication Engineering from the Amrita School of Engineering, Coimbatore on May 2017. . I currently work as a Software Test Engineer, Testing Virtual Network Functions(VNF) and solve Cloud based challenges related to Telecom Industry at Ericsson, Bangalore, Karnataka. . I am interested in Deep Learning and Computer Vision with specific focus on GANs, Adversarial Machine learning and Federated Learning. I am inclined towards working on projects that involve working on the intersection of these fields. I enjoy playing Table Tennis, Fixing Broken Electronics, Watching Movies and Programming. . Skills . Python, Flask, RobotFramework, Selenium, Pandas, Scikit-learn | C, MATLAB, Libsvm | Deep Learning Frameworks: Keras, Tensorflow 2+, Pytorch | Fundementals of AWS and GCP , Openstack | Git, REST API | Automation of Web GUI and API | . MOOCS . (WIP) AI for Medicine Specialization (Coursera) | (WIP) TensorFlow: Data and Deployment Specialization (Coursera) | Deep Learning Specialization (Coursera) | TensorFlow in Practice Specialization (Coursera) | Pratical Deep Learning for Coders - Fast.AI (V2 &amp; V3) | Deep Learning from the Foundations - Fast.AI V3 | . Publications . Gokkul Nath T.S., Sudheesh P., Jayakumar M Tracking Inbound Enemy Missile for Interception from Target Aircraft Using Extended Kalman Filter, Mueller P., Thampi S., Alam Bhuiyan M., Ko R., Doss R., Alcaraz Calero J. (eds) Security in Computing and Communications. SSCC 2016. Communications in Computer and Information Science, vol 625. Springer, Singapore. |       .",
          "url": "https://blog.gokkulnath.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "About Me",
          "content": "If you are not redirected automatically, follow this link to get to know about me. .",
          "url": "https://blog.gokkulnath.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
      ,"page6": {
          "title": "News!",
          "content": "Current Works . Achievements . Ericsson Impact AWARD - 2018 from Head of Business Area Managed Services | 1 out of 15 Top Performers for 2018-2019 in BMAS Service Assurance MS IT &amp; ADM Unit. | . Activities! . Ericsson Internal Trainer for Machine Learning and Python Competence development. Trained more than 50 Colleagues and enabled them to apply machine learning to Telecom and IT industry. | External Internship Participant @Inkers.ai - EIP 2 and EIP 3 | AI Saturdays : Ambassador for Bangalore Chapter: Cycle 1 Testimonial | Fast.AI international Fellow | One of the 15 Selected Outreach Participant to attend Computational Science Symposium (CSS-2017) | .",
          "url": "https://blog.gokkulnath.ml/News/",
          "relUrl": "/News/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.gokkulnath.ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}